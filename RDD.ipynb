{"cells":[{"cell_type":"markdown","source":["What is Action in RDD?\n\nAn action is one of the ways of sending data from Executer to the driver. Executors are agents that are responsible for executing a task. While the driver is a JVM process that coordinates workers and execution of the task."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f7d3d18-5976-4a20-8b72-dfab58a7893e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark import SparkContext ##we will use SparkContext for RDD, SparkSession used whiel creating the DataFrame \n\nsc = SparkContext.getOrCreate()\n\nprint(sc)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e04bd5e0-ad35-4727-ab1f-251a22ad32b2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["<SparkContext master=local[8] appName=Databricks Shell>\n"]}],"execution_count":0},{"cell_type":"code","source":["my_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d4fcee55-2778-4b44-91e0-c6cb121a1833","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(my_rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fcf4a08f-ceb2-4c01-a124-9a08353a72e9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[9]: pyspark.rdd.RDD"]}],"execution_count":0},{"cell_type":"code","source":["type(my_rdd.collect())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"380e8d64-ec66-4cce-b8fb-ac053ba84596","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[10]: list"]}],"execution_count":0},{"cell_type":"code","source":["my_rdd.take(3)   #First 3 values it display"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e6aa8492-5d10-49e0-ac58-9c6af88e59c2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[7]: [1, 2, 3]"]}],"execution_count":0},{"cell_type":"code","source":["my_rdd.top(3)   #Last 3 values"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d4a5d78-f659-48a0-a38b-08359467512a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[9]: [14, 13, 12]"]}],"execution_count":0},{"cell_type":"code","source":["my_rdd.min()   #1\nmy_rdd.max()   #14\nmy_rdd.sum()   #105\nmy_rdd.count()  #14\nmy_rdd.mean()   #7.5\nmy_rdd.stdev()  #4.03"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d0362b32-f79a-48f6-b0a1-b9f193b64c7f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[16]: 4.031128874149275"]}],"execution_count":0},{"cell_type":"code","source":["my_rdd.takeSample(True,5)  #it will select sample values with repeated values \n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6ef67d38-8446-4f45-98a3-46b98d9feb02","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[17]: [10, 8, 7, 11, 8]"]}],"execution_count":0},{"cell_type":"code","source":["my_rdd.takeSample(False,5) # it will select sample values with out repeated values"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c6197729-09e0-402b-98b5-84987cc65105","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[18]: [12, 1, 6, 5, 14]"]}],"execution_count":0},{"cell_type":"markdown","source":["No. RDD Action  Expecting Result\n\n1 collect()        Convert RDD to in-memory list\n\n2 take(3)       First 3 elements of RDD\n\n3 top(3)           Top 3 elements of RDD\n\n4 count()              Find total no of values in RDD.\n\n5 min()      Find minimum value from the RDD list\n\n6 max()     Find maximum value from the RDD List\n\n7 sum()   Find element sum (assumes numeric elements)\n\n8 mean() Find element mean (assumes numeric elements)\n\n9 stdev() Find element deviation (assumes numeric elements)\n\n10 takeSample(withReplacement=True,3) Create sample of 3 elements with replacement"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5862d140-6590-465b-9564-51729c2867a4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["No. RDD Action  Expecting Result      \n\n11 reduce() Reduce is a spark action that aggregates a data set (RDD) element using a function.   \n\n12 countByKey() Count the number of elements for each key, and return the result to the master as a dictionary.      \n\n13 CountByValue() Return the count of each unique value in this RDD as a dictionary of (value, count) pairs.      \n\n14 fold() Aggregate the elements of each partition      \n\n15 range() Create a new RDD of int containing elements from start to end (exclusive)      \n\n16 variance() Compute the variance of this RDD’s elements.      \n\n17 sampleVariance() Compute the sample variance of this RDD’s elements (which corrects for bias in estimating the variance by dividing by N-1 instead of N).      \n\n18 saveAsTextFile() Save this RDD as a text file, using string representations of elements.      \n\n19 saveAsPickleFile() Save this RDD as a SequenceFile of serialized objects      \n\n20 Stats() Stats will give complete information count, min, max,  stdev and mean"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e8b38a7-1020-4de4-96ee-942b53d61398","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3d32b09-7712-47a4-9e94-a95088217952","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#reduce\nrdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\nrdd.reduce(lambda a,b : a+b)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a41acc5a-e103-46b5-a247-e1e123ae68e8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[21]: 55"]}],"execution_count":0},{"cell_type":"code","source":["#countByValue\n\nrdd = sc.parallelize([1,3,1,2,3,1,3,1,3,2,4,2,5,4,3])\nrdd.countByValue()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8396a60-7b8a-40fa-84d5-a30d66e0242f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[23]: defaultdict(int, {1: 4, 3: 5, 2: 3, 4: 2, 5: 1})"]}],"execution_count":0},{"cell_type":"code","source":["#countByKey\nx = sc.parallelize([('A', 1), ('B', 2),('A', 3),('B', 4),('A', 5)])\n\nx.countByKey()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6937468-937d-4d39-aee9-9a84c62d26b8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[24]: defaultdict(int, {'A': 3, 'B': 2})"]}],"execution_count":0},{"cell_type":"code","source":["#Fold Aggregate the elements of each partitions, and then the results for all the partitions \nfrom operator import add \nx_fold = sc.parallelize([1,2,3,4,5])\nx_fold.fold(0, add)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3db781c6-8cc0-4214-9b8c-f6deeeebcaef","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[26]: 15"]}],"execution_count":0},{"cell_type":"code","source":["#Range \n\nx_range = sc.parallelize(range(1,10))\n\nx_range.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eaa56b31-942d-4084-9a50-369be718fa88","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[29]: [1, 2, 3, 4, 5, 6, 7, 8, 9]"]}],"execution_count":0},{"cell_type":"code","source":[" #variance \n    \nx_var = sc.parallelize([1,2,3,4,5,6,7,8,9])\n\nx_var.variance()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fbeded0c-672c-4c41-a2a7-f129e309b0f2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[30]: 6.666666666666667"]}],"execution_count":0},{"cell_type":"code","source":["#sampleVariance\nx_var.sampleVariance()\n "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f306f677-9758-4cc7-934f-b8405c17c188","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[31]: 7.5"]}],"execution_count":0},{"cell_type":"code","source":["#Save as textFile , #Save as pickel file\n\npass"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4714ec1-79d2-4568-af06-7fec0de19a73","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#stats\n\nsc.parallelize([1,2,3,4,5]).stats()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ac1cc1e6-59b4-4b20-97f6-c7b853ff6033","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[35]: (count: 5, mean: 3.0, stdev: 1.4142135623730951, max: 5.0, min: 1.0)"]}],"execution_count":0},{"cell_type":"code","source":["x_map = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\ny_map = x_map.map(lambda x: (x, x**2))\nprint(x_map.collect())\nprint(y_map.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"164529f8-07f3-4b07-8cfe-20495cab27f6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25), (6, 36), (7, 49), (8, 64), (9, 81), (10, 100)]\n"]}],"execution_count":0},{"cell_type":"code","source":["x_fil = sc.parallelize(range(1,12))\ny_fil = x_fil.filter(lambda x: x%2 == 0)\ny_fil.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b9b7a6d-8dbf-420b-9aec-3ddcfe687869","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[45]: [2, 4, 6, 8, 10]"]}],"execution_count":0},{"cell_type":"code","source":["#flatmap : function should return a seq instead of single item \n\nx_fm = sc.parallelize(range(1,13))\ny_fm=x_fm.flatMap(lambda x: (x**2 , 100*x))\ny_fm.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6524267-66ec-48cc-b5ba-f5347941a93a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[48]: [1,\n 100,\n 4,\n 200,\n 9,\n 300,\n 16,\n 400,\n 25,\n 500,\n 36,\n 600,\n 49,\n 700,\n 64,\n 800,\n 81,\n 900,\n 100,\n 1000,\n 121,\n 1100,\n 144,\n 1200]"]}],"execution_count":0},{"cell_type":"code","source":["#mapPartitions: similar to map but run separetly on each partition(block) of the rdd\n\nx_par=sc.parallelize(range(1,16),3)\ndef func(x):\n    yield sum(x)\n    \ny_par=x_par.mapPartitions(func)\n\n#glom flattens elements on the same partition\nprint(x_par.glom().collect())\n\nprint(y_par.glom().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"26f31323-bf3e-4ebc-b11d-9edd76d79ecc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]]\n[[15], [40], [65]]\n"]}],"execution_count":0},{"cell_type":"code","source":["#mapPartitionsWithIndex\n\n\nx_parIndex=sc.parallelize(range(1,16),3)\n\ndef funcIndex(partitionIndex, iterator): \n    yield (partitionIndex,sum(iterator))\n\ny_pari = x_parIndex.mapPartitionsWithIndex(funcIndex)\n\nprint(y_pari.glom().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"64150b03-1489-40ec-9383-557e3eedd100","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[[(0, 15)], [(1, 40)], [(2, 65)]]\n"]}],"execution_count":0},{"cell_type":"code","source":["#sample ==> sample(withReplacement, fraction, seed= None)\n\nx = sc.parallelize(range(1,10))\ny= x.sample(False,0.2)   #If it's True, values are may or may not repeat. for False, value won't be repeated\n\nprint(x.collect())\nprint(y.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ceaba45a-29c4-46d8-a584-8573fe218602","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[1, 2, 3, 4, 5, 6, 7, 8, 9]\n[4, 8]\n"]}],"execution_count":0},{"cell_type":"code","source":["#Union \n\nrdd_1 = sc.parallelize(range(0,5))\nrdd_2 = sc.parallelize(range(3,8))\n\n#Intersection\nrdd_1.union(rdd_2).collect()\nrdd_1.intersection(rdd_2).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b580f284-8d54-48b6-9111-17790e507238","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[64]: [3, 4]"]}],"execution_count":0},{"cell_type":"code","source":["#distinct \n\nx = sc.parallelize(['A','A','B','B','C','D','E'])\n\ny=x.distinct()\n\nprint(y.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b58d1217-9150-43a2-8f6b-b3ec09951872","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["['B', 'C', 'A', 'D', 'E']\n"]}],"execution_count":0},{"cell_type":"code","source":["#groupByKey\n\nx = sc.parallelize([('B',2),('A',3),('C',3),('A',12),('D',3)])\n\ny=x.groupByKey()\n\nx.collect()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"62299c89-4637-4963-bc3d-cc4825c2c77f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-3876523344670179>\"\u001B[0;36m, line \u001B[0;32m13\u001B[0m\n\u001B[0;31m    print('',end=\"\\n)\u001B[0m\n\u001B[0m                         ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m EOL while scanning string literal\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: EOL while scanning string literal (<command-3876523344670179>, line 13)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-3876523344670179>\"\u001B[0;36m, line \u001B[0;32m13\u001B[0m\n\u001B[0;31m    print('',end=\"\\n)\u001B[0m\n\u001B[0m                         ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m EOL while scanning string literal\n"]}}],"execution_count":0},{"cell_type":"code","source":["#Cache and persist  https://www.youtube.com/watch?v=Q2qpCGtIQgU&list=PL50mYnndduIHGS49Q_tve1f7aW4NHjvgQ&index=7&ab_channel=TechLake\n\n#Pyspark Tutorial: 7 TechLake \n\n# rdd_emp = sc.read.csv(\"file\")\n\n# rdd_emp.cache()\n\n# rdd_emp.unpersist()\n\n# rdd.persist(pyspark,StorageLevel.MEMORY_AND_DISK_SER)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"71e30dd5-385d-4b6e-8e7e-acdfa7815bc3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Broadcast variable\nbroadcast_v = sc.broadcast([1,2,3,4,5,6,7])\nprint(type(broadcast_v))\n\nprint(broadcast_v.value)\n#broadcast_v.persit()\n#broadcast_v.unpersist()\nbroadcast_v.destroy"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9b3de961-dad4-4912-acd7-33388b6adcdd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["<class 'pyspark.broadcast.Broadcast'>\n[1, 2, 3, 4, 5, 6, 7]\nOut[79]: <bound method Broadcast.destroy of <pyspark.broadcast.Broadcast object at 0x7fe17c701a90>>"]}],"execution_count":0},{"cell_type":"code","source":["help(broadcast_v)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6633a665-0e4a-4151-bce3-84574dee5f72","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Help on Broadcast in module pyspark.broadcast object:\n\nclass Broadcast(typing.Generic)\n |  Broadcast(sc: Optional[ForwardRef('SparkContext')] = None, value: Optional[~T] = None, pickle_registry: Optional[ForwardRef('BroadcastPickleRegistry')] = None, path: Optional[str] = None, sock_file: Optional[BinaryIO] = None)\n |  \n |  A broadcast variable created with :meth:`SparkContext.broadcast`.\n |  Access its value through :attr:`value`.\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.context import SparkContext\n |  >>> sc = SparkContext('local', 'test')\n |  >>> b = sc.broadcast([1, 2, 3, 4, 5])\n |  >>> b.value\n |  [1, 2, 3, 4, 5]\n |  >>> sc.parallelize([0, 0]).flatMap(lambda x: b.value).collect()\n |  [1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n |  >>> b.unpersist()\n |  \n |  >>> large_broadcast = sc.broadcast(range(10000))\n |  \n |  Method resolution order:\n |      Broadcast\n |      typing.Generic\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, sc: Optional[ForwardRef('SparkContext')] = None, value: Optional[~T] = None, pickle_registry: Optional[ForwardRef('BroadcastPickleRegistry')] = None, path: Optional[str] = None, sock_file: Optional[BinaryIO] = None)\n |      Should not be called directly by users -- use :meth:`SparkContext.broadcast`\n |      instead.\n |  \n |  __reduce__(self) -> Tuple[Callable[[int], ForwardRef('Broadcast[T]')], Tuple[int]]\n |      Helper for pickle.\n |  \n |  destroy(self, blocking: bool = False) -> None\n |      Destroy all data and metadata related to this broadcast variable.\n |      Use this with caution; once a broadcast variable has been destroyed,\n |      it cannot be used again.\n |      \n |      .. versionchanged:: 3.0.0\n |         Added optional argument `blocking` to specify whether to block until all\n |         blocks are deleted.\n |      \n |      Parameters\n |      ----------\n |      blocking : bool, optional\n |          Whether to block until unpersisting has completed\n |  \n |  dump(self, value: ~T, f: <class 'BinaryIO'>) -> None\n |  \n |  init_with_process_isolation(self, sc, value, pickle_registry)\n |      Initializes the broadcast variable through trusted file path. This modifies the existing\n |      workflow in a secure way which would still work with process isolation. The broadcast value\n |      is also stored as an attribute to avoid needing to load the value from the file.\n |  \n |  load(self, file: <class 'BinaryIO'>) -> ~T\n |  \n |  load_from_path(self, path: str) -> ~T\n |  \n |  unpersist(self, blocking: bool = False) -> None\n |      Delete cached copies of this broadcast on the executors. If the\n |      broadcast is used after this is called, it will need to be\n |      re-sent to each executor.\n |      \n |      Parameters\n |      ----------\n |      blocking : bool, optional\n |          Whether to block until unpersisting has completed\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  value\n |      Return the broadcasted value\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __orig_bases__ = (typing.Generic[~T],)\n |  \n |  __parameters__ = (~T,)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from typing.Generic:\n |  \n |  __class_getitem__(params) from builtins.type\n |  \n |  __init_subclass__(*args, **kwargs) from builtins.type\n |      This method is called when a class is subclassed.\n |      \n |      The default implementation does nothing. It may be\n |      overridden to extend subclasses.\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###RDD Transformation\\\n\nChanges data from one format to another\neg: map, filter, flatmap \n\nLazy execution - Delay execution untill finds an execution so that it can prepare optimized lineage(Spark internel coe pipeline)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cb043498-73d1-4d32-ae19-fea4a5aff36e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#map #filter\n\nx_map = sc.parallelize(range(1,11))\n\nx_lambda = x_map.map(lambda x: (x,x**2))\n\n# print(x_map.collect())\n# print(x_lambda.collect())\n\n#filter\n# x_filter = x_map.filter(lambda x: x> 5)\n# print(x_filter.collect())\n\n#flapmap \n\n# x_flatmap = x_map.flatMap(lambda x: (x, x**3))\n# print(x_flatmap.collect())\n\n#mappartitions\n\n# x_mapp = x_map.repartition(2)\n\n# def f(iterator):\n#     yield sum(iterator)\n    \n# x_mapp.mapPartitions(f).glom().collect()\n\n#groupByKey \n\nx = sc.parallelize([('A',1),('B',2),('C',3),('A',23),('B',1),('E',12)])\nx.groupByKey().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"983f99f3-ffdf-43ce-be9e-604d80cf3949","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[11]: [('B', <pyspark.resultiterable.ResultIterable at 0x7f7efe64ae80>),\n ('C', <pyspark.resultiterable.ResultIterable at 0x7f7efe50ae20>),\n ('A', <pyspark.resultiterable.ResultIterable at 0x7f7efe50ad60>),\n ('E', <pyspark.resultiterable.ResultIterable at 0x7f7efe50ad90>)]"]}],"execution_count":0},{"cell_type":"markdown","source":["####cache and persist  and RDD unpersist\n\nBoth caching and persisting are used to save the Spark RDD, Dataframe, and Dataset’s. But, the difference is, RDD cache() method default saves it to memory (MEMORY_ONLY) whereas persist() method is used to store it to the user-defined storage level.\n\nStorage-level :\n\nMEMORY_ONLY\n\nMEMORY_AND_DISK\n\nMEMORY_ONLY_SER\n\nMEMORY_AND_DISK_SER\n\nDISK_ONLY\n\nOFF_HEAP\n\nSaprk automatically monitors cache usage on each node and drops out old data partitions in a least recently  used LRU fashion. If you would like to remove and rdd instead of waiting for it to fall out of the cache, use RDD.unpersist() method\n\n```\nrdd.persist(pyspark.StorageLevel.DISK_ONLY)\nrdd.unpersist()\nrdd.cache()\n```\n\n######Advantages : \nTime and cost efficient\nlessen the execution time"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bbf6fc04-f93a-40a2-b0b5-eb9c01c66d38","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["####Broadcast Vraibles \n\nBroadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. \n\nThey can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\n7\n```\nbroadcast_v = sc.parellize([1,2,3,4,5,6,])\nbraodcast_v.value\nbroadcast_v.unpersist()\nbroadcast_v.destroy()\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c16855cf-d74e-4f78-8d44-ba211b8a21b4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["####Accumulators\n\nAccumulators are varibles that are only \"added\" to through an associative and ccommutative operation and can therfore be efficiently supported in parellel\n\n```\naccum_v = sc.accumulator(2)\nsc.parellize([1,2,3,4,5,6,7,8,9]).foreach(lambda x:accum_v.add(x))\naccum_v.value\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"afb6e897-0bac-4ff7-a97a-3802db2cfdfb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58b6e54e-cbca-44bd-b20a-ef5729f71995","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["accum_v = sc.accumulator(2)\nsc.parallelize([1,2,3,4,5,6,7,8,9]).foreach(lambda x:accum_v.add(x))\naccum_v.value"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ae2fbea8-3907-4ce7-a38b-9117b28f2c9d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[19]: 47"]}],"execution_count":0},{"cell_type":"markdown","source":["###RDD transformation  Joins \n \n Join\n \n leftOuterJoin\n \n rightOuterJoin\n \n fullOuterJoin\n \n cartesian()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4646baf0-d57f-476d-b8ba-c6afd7f8f382","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2dbeacba-5e18-4607-b87a-54e9888d9b19","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["####Diff between RDD, Dataframe and Dataset(Java and Scala)\n\n#######RDD Features:\n\nDistributed collection\n\n1. immutable\n2. Fault tolerant\n3. Lazy evaluations\n4. Function Transformation ==> Transformation and Action\n5. Data processing formats ==> Structured and Unstructured data\n6. Programming Languages supported  => Java, Scala, R and Python \n\n######DataFrame Features:\n\n1. Distributed collection of row object\n2. Data processing (csv, json, parquet and etc)\n3. OPtimization using catalyst optimizer\n4. Hive compatibility\n5. Tungsten\n6. Supported in Java,Scala,R and python \n\n######Dataset features\n1. Provides best of both RDD and Dataframe\n2. Encoders\n3. Type safety\n4. supported in Java and Scala"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3faaf3c5-0590-4fae-a222-09f0d4428eb5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["####Disadvantages RDD, Dataframe and Dataset(Java and Scala)\n\n######RDD\n1. We have to optimize each and every RDD. \n2. RDD's don't infer the schema of the data ingested therefore we have to specify it \n\n######Dataframe\nThe main drawback of Dataframe API is that it doesn't support compile time safely, as a result, the user is limited in case the structure of the data is unknowm\n\n######Dataset\nThey require typecasting into strings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"815fb24f-9bb4-4eb5-a5ac-b68b173493c7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["####Read and write json file \n\n```\nemp_json = spark.read.json(\"test.json\")\nemp_text = spark.read.text(\"test.json\")  #dispaly the value emp_text , so that we will get the data how it is , like json or csv and etc\n\ndisplay(emp_json)\n\n###Creating sample json file \n\ndbutils.fs.put(\"/tmp/test.json\", \"\"\"\n{\"string\":\"string1\", \"int\":1, \"array\":[1,2,3], \"dict\": {\"key\":\"value1\"}}\n{\"string\":\"string2\", \"int\":2, \"array\":[3,4,6], \"dict\": {\"key\":\"value2\"}}\n{\"string\":\"string3\", \"int\":3, \"array\":[7,8,9], \"dict\": {\"key\":\"value3\"}}\n\"\"\", True)\n```\n\n\nUsing SQL Temporary view on Json files for reading  json data \n\n```\n%sql\nCREATE TEMPORARY VIEW emp_json_table\nusing json \noptions (path=\"/temp/emp_details.json\")\n \nselect * from emp_json_table\n\n```\n\n\nif the data is present in multiline \n\n```\n{\"string\":\"string1\", \n  \"int\":1, \n    \"array\":\n          [1,2,3], \n     \"dict\": \n         {\"key\":\"value1\"}}\n{\"string\":\"string2\", \"int\":2, \"array\":[3,4,6], \"dict\": {\"key\":\"value2\"}}\n{\"string\":\"string3\", \"int\":3, \"array\":[7,8,9], \"dict\": {\"key\":\"value3\"}}\n\"\"\", True)\n```\n\nthen we need to use *multiline=True* option while reading the file\n```\nemp_json = spark.read.option('multiline',\"true\").json(/tmp/emp_data.json)\n```\n\n\nRDD Storing json object per string\n```\nmy_data = ['{\"name\":\"Ravi\", \"add\":{\"state\":\"Karnatake\",\"city\":\"Bengaluru\"}}']\nmy_rdd = sc.parellelize(my_data)\nmy_df = spark.read.json(my_rdd)\nmy_df.printSchema()\n```\n\nWriting into file\n```\n\nemp_json.write.json(\"/tmp/emp_data.json\".mode='append')\n\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"29e3ccb0-1708-4206-aefc-74052c201fb4","inputWidgets":{},"title":"JSON"}}},{"cell_type":"markdown","source":["##reading and writing Excel file \n\nInstall library files for excel file \n\nDatabricks account-> clusters-> libraries->install new-> click maven in coordinates -> search for `spark-excel` then select latest version ->  com.crealytics:spark.excel_2.12:0.13.5\n\n```\nexcel_read_data = spark.read.format('com.crealytics.spark.excel').option(\"header\",\"true\").option(\"inferschema\",\"true\").load('test.xlsx')\n\nexcel_read_data.select('*').write.format('com.crealytics.spark.excel').option(\"header\",\"true\").option(\"inferschema\",\"true\").save(\"emp_data_DF.json\")\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0045d447-55a4-453b-805b-fc88bfe21aaf","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##Interview Questions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c58d2ac8-0565-4f66-9c7f-8f0131896631","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["####How to handle bad records in spark and those types \n\nThere are three types of modes available while reading and creating Dataframe\n\nDealing with bad records verify correctness of the data when reading csv file with specified schema, it is possible that the data in the files does not match the schema \n\n##########mode:behaviour\n1. PERMISSIVE : Includes corrupt records in a \"__corrupt_record\" column by default (defauulty , it's set true while reading json file )\n2. DROPMALFORMED : Ignore all corrupted records \n3. FAILFAST : Throws an exception when it meets corrupted records \n\n\nbadRecordsPath Option to store rejected records in externel loctaion by proving the error message \n1. use the badRecordsPath option to save corrupt\n2. records to the directory specified by the corruptPathvariable now\n3. unable to find input file then it will log error information into `badRecordsPath`\n4. if 'badRecordsPath' is specified, mode is not allowed to set."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4528c740-08cf-4dd2-be6c-8e21583729ac","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###How to get all available dataframes in pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d1521668-2bce-433f-a7d4-9aa6263dbbf2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Answer\nfrom pyspark.sql import DataFrame\nprint([k for (k,v) in globals().items() if isinstance(v,DataFrame)])   #globals() will provide all the used spark varibles\n\n# print(globals())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d146f3b2-bb88-4936-9416-deae71ecaa6e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[]\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###How to track add source file name in one of column in Dataframe? \n\nanswer : using input_file_name() function and withColumn() function we can add new column for filenames\n\ndf=spark,read,csv(/tmp/*.csv)\n\ndf.withColumn(\"file_name\", input_file_name()).limit(3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19b7717e-10a5-4be4-9251-08442368128e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###Get no of rows on each file in a dataframe ? \n\nans : using input_file_name() function and groupBy transformation we can achive no of rows on each file\n\nfrom pyspark.sql.functions import input_file_name\n\ndf=spark,read,csv(/tmp/*.csv)\n\ndf.withColumn(\"file_name\", input_file_name()).groupBy(\"file_name\").count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dc4b5627-6b0e-46ec-bd6a-e1a1b9bea523","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###How to add PartitionId in dataframe\nAns: using spark_partition_id function and withColumn we can get the partitionId and add into DataFrame \n\nfrom pyspark.sql.functions import spark_partition_id\n\ndf.withColumn('partitionId', spark_partition_id()).groupBy('')\n\n\n######Row count by partitionId\n\ndf.withColumn('partitionId', spark_partition_id()).groupBy('partitionId').count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a082718-65a4-40cb-98dd-5672bcb156b3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###How to add Sequence generated surrogate key as a column in dataframe\n\nans : using monotonically_increasing_id() or hash() functions we can generate sequence or surrogate key\n\n```\nfrom pyspark.sql.functions import monotonically_increasing_id\n\ndf.withColumn(\"key\",monotonically_increasing_id())\n```\n\nBut above have some disadvantage when we remove the particular row and when we reporcess the data , it will chnage the id i.e., we will get different id again and again \n\n\n#####Using `MD5`\n\n```\nfrom pyspark.sql.functions import md5\n\ndf.withColumn(\"Key\",md5(\"EMPNO\"))  #based on specific column value, it will providie unique value thiugh we process again and again\n```\n\nBut above has disavnatage like, if the data is very huge , then there might be chance of duplicate value  \n\n#####Using `Sha2`\n\n```\nfrom pyspark.sql.functions import sha2\n\ndf.withColumn(\"Key\",sha2(\"EMPNO\",256))  #based on specific column value, it will providie unique value thiugh we process again and again\n```\n\n\nRow_number is also we can use but it is not suggestable"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8a1e75f5-d1a8-412c-aea2-3eb58666a478","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###What is Global Temporary view and Temporary view\n\nans:  \n   - Temporary views in spark sql are session scoped and will disappear if the sessions that creates it terminates\n\n   - If you want to have temporary view that is `shared among all sessions and keep alive untill the spark application terminates`. \n        You can create a `global temprary view` (server level)\n\n   - Global temporary view is tied to system preserved database global_temp, and we must use the qualified name to refer it. i.e., \n          ```select * from global_temp.view1```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"971e64c2-bd5f-4d25-ab50-b366989fb9f8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###How to get list of databases, tables and columns using spark.catalog\n\nans:\n```\ndisplay(spark.catalog.listDatabases())\ndisplay(spark.catalog.listTables(\"database_name\"))\ndisplay(spark.catalog.listcolumns(\"database_name\",\"table_name\"))\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c4031c8-8fe8-4acf-bdda-8b49fac8eabc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###How partitions are created?\n\n- Spark's tasks process data as partitions read from disk into memory. Data on disk is lead out in chunks or contiguous file blocks, depends on store.By default, file blocks on data storage range in size from 64MB to 128MB . for example, on HDFS and s3 the default size is 128MB. A contiguous collection of these blocks constitutes a partitions\n\n```\nspark.conf.get(\"spark.sql.files.maxPartitionBytes\") \nspark.conf.set(\"spark.sql.files.maxPartitionBytes\", \"300MB\")\n```\n\nRDD will create 8 partitions by default when we use parallelize operation \nRDD will create 2 partitions by default when we use textFile opearion\n```\nrdd=sc.parallelize([1,2,3,4])\nrdd.getNumPartitions()  #8\n```\n\n```\nrdd=sc.textFile(\"/tmp/a.txt\")\nrdd.getNumPartitions()   #2\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"55a97441-3065-462b-b6e3-2198aaa0cba1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["rdd=sc.parallelize([1,2,3,4])\nrdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b82c9bc-8809-4f0f-998b-1df495ce7f8d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[3]: 8"]}],"execution_count":0},{"cell_type":"markdown","source":["###How to get shuffle partitions count from spark configuration?\n```\nspark.conf.get(\"spark.sql.shuffle.partitions\")   #default is 200 \nspark.conf.set(\"spark.sql.shuffle.partitions\", \"20\")  ##setting it to 20\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1da3306-3f42-4eaa-acea-34caeb2054d6","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###Path Global Filter\n\n- `pathGlobalFilter` is used to only include files with file names matching the pattern   \n\n###Recursive File lookup \n\n- `pathGlobalFilter` is used to recursively load files and it disables partition inferring\n- it's default value is false. if data source explicitly specifies the partitionSpec when recursiveFileLookup is true, exception will be thrown"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"63bf07c2-6d07-44f3-8b83-c82849d2dd75","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# from pyspark.sql.functions import input_file_name\n# df=spark.read.option(\"pathGlobalFilter\", \"sample*.json\").csv(\"dbfs:/Filestore/tables\") \\\n#     .withColumn(\"file_name\", input_file_name())\n\n# display(df.select(\"file_name\").distinct())\n\n# #recursive\n# df=spark.read.option(\"recursiveFileLookup\",\"true\").csv(\"/FileStore/tables/emp/\", header = True ).withColumn(\"file_name\", input_file_name())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ccb424e8-6418-44a1-a646-6aa242719504","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-2715617520755858>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0minput_file_name\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pathGlobalFilter\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"sample*.json\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dbfs:/Filestore/tables\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"file_name\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_file_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"file_name\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistinct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    533\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 535\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    536\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path does not exist: dbfs:/Filestore/tables","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Path does not exist: dbfs:/Filestore/tables","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-2715617520755858>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0minput_file_name\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pathGlobalFilter\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"sample*.json\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"dbfs:/Filestore/tables\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"file_name\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_file_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mdisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"file_name\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistinct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mcsv\u001B[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001B[0m\n\u001B[1;32m    533\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    534\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 535\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_spark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonUtils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoSeq\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    536\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path does not exist: dbfs:/Filestore/tables"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2dfe108-2d31-449f-8d43-6c7e50f57d14","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"RDD","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4266011270488321}},"nbformat":4,"nbformat_minor":0}

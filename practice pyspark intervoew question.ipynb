{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95b0cead-9499-44a8-8683-ef3cc330ebee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "1. Pyspark :Joining DataFrames in PySpark Without Duplicate Columns \n",
    "2. sql: find out the customer who consecutive ordered particular product within 3odays of first one bought date\n",
    "3. leftanti join synatx.(scenario, identify which join and write syntax) \n",
    "4. create a datafrmae with below, where incentive is string. Add new column with the resultant of salary*incentive(atfer converting 'incentive'  to Int)\n",
    "\n",
    "dept     salary  incentive\n",
    "\n",
    "Sales     3000   10.00\n",
    "\n",
    "Sales     4600   10.02\n",
    "\n",
    "Sales     4100   10.15\n",
    "\n",
    "6. Window function usage, use pyspark dataframe below, find the second rank of salary of each dept  (window function , second lowest _salary for each dept )\n",
    "\n",
    "dept     salary\n",
    "\n",
    "Sales     3000\n",
    "\n",
    "Sales     4600\n",
    "\n",
    "Sales     4100\n",
    "\n",
    "df.groupBy('dept').\n",
    "\n",
    "7. How to debug pyspark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9972f72-3763-42bd-a2ce-c7155d7b9cf2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "* **Question.1: Window function usage, use pyspark dataframe below, find the second rank of salary of each dept  (window function , second lowest _salary for each dept )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a89d1f-53e6-499a-b448-f9f92726b698",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be9eb3dc-f4bf-4e0d-83b9-4b8ae03d6b2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employees_Salary = [(\"James\", \"Sales\", 2000,'10'),(\"sofy\", \"Sales\", 3000,'10'),(\"Laren\", \"Sales\", 4000, '10'),(\"Kiku\", \"Sales\", 5000, '10'),(\"Sam\", \"Finance\", 6000, '10'),(\"Samuel\", \"Finance\", 7000, '10'),(\"Yash\", \"Finance\", 8000, '10'),(\"Rabin\", \"Finance\", 9000, '10'),(\"Lukasz\", \"Marketing\", 10000, '10'),(\"Jolly\", \"Marketing\", 11000, '10'),(\"Mausam\", \"Marketing\", 12000, '10'),(\"Lamba\", \"Marketing\", 13000, '10'),(\"Jogesh\", \"HR\", 14000, '10'),(\"Mannu\", \"HR\", 15000, '10'),(\"Sylvia\", \"HR\", 16000, '10'),(\"Sama\", \"HR\", 17000, '10'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91293022-18b7-421f-ac3d-f17e7c5a686e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n+------+---------+------+----------+---+\n|  Name|     Dept|Salary|incentives| rr|\n+------+---------+------+----------+---+\n|Samuel|  Finance|  7000|        10|  2|\n| Mannu|       HR| 15000|        10|  2|\n| Jolly|Marketing| 11000|        10|  2|\n|  sofy|    Sales|  3000|        10|  2|\n+------+---------+------+----------+---+\n\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(employees_Salary,schema=['Name', 'Dept','Salary', 'incentives'])\n",
    "# df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "print(type(df))\n",
    "df.createOrReplaceTempView('emp')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          with CTE_rank as (\n",
    "              select * , dense_rank() over(partition by Dept  order by salary) as rr from emp\n",
    "          )\n",
    "\n",
    "          select * from cte_rank where rr = 2       \n",
    "          \"\"\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485160e0-ee8c-44af-894c-45cb996a563c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+\n|  Name|     Dept|Salary| rn|\n+------+---------+------+---+\n|Samuel|  Finance|  7000|  2|\n| Mannu|       HR| 15000|  2|\n| Jolly|Marketing| 11000|  2|\n|  sofy|    Sales|  3000|  2|\n+------+---------+------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "\n",
    "window=Window.partitionBy('Dept').orderBy('Salary')\n",
    "dd=df.select(df.Name, df.Dept, df.Salary, dense_rank().over(window).alias('rn'))\n",
    "dd.filter(\"rn=2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf3b5e6a-868b-4e4e-95b5-dff711d6427e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Dept: string (nullable = true)\n |-- Salary: long (nullable = true)\n |-- incentives: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ca5be58-1d17-41f2-97e9-445825651b1a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "* **2. Question: new column with the resultant of salary*incentive(atfer converting 'incentive'  to Int)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549a3e64-59b0-4fc5-b745-3a4b0cbb6449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Dept: string (nullable = true)\n |-- Salary: long (nullable = true)\n |-- incentives: string (nullable = true)\n |-- New_incentive_int_type: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1=df.withColumn('New_incentive_int_type', df.incentives.cast('int'))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99845655-03c5-4cbc-947d-738b029ae42b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+----------------------+----------+\n|  Name|     Dept|Salary|New_incentive_int_type|new-Salary|\n+------+---------+------+----------------------+----------+\n| James|    Sales|  2000|                    10|     20000|\n|  sofy|    Sales|  3000|                    10|     30000|\n| Laren|    Sales|  4000|                    10|     40000|\n|  Kiku|    Sales|  5000|                    10|     50000|\n|   Sam|  Finance|  6000|                    10|     60000|\n|Samuel|  Finance|  7000|                    10|     70000|\n|  Yash|  Finance|  8000|                    10|     80000|\n| Rabin|  Finance|  9000|                    10|     90000|\n|Lukasz|Marketing| 10000|                    10|    100000|\n| Jolly|Marketing| 11000|                    10|    110000|\n|Mausam|Marketing| 12000|                    10|    120000|\n| Lamba|Marketing| 13000|                    10|    130000|\n|Jogesh|       HR| 14000|                    10|    140000|\n| Mannu|       HR| 15000|                    10|    150000|\n|Sylvia|       HR| 16000|                    10|    160000|\n|  Sama|       HR| 17000|                    10|    170000|\n+------+---------+------+----------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "df1.withColumn('new-Salary', col('salary')* col('New_incentive_int_type')).drop('incentives').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8115ab-c243-4f88-8d44-a0112aafddf3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "* **3.Question:leftanti join synatx.(scenario, identify which join and write syntax)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa4ed26f-8fe0-4552-a43b-37c31f5ba917",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+--------+-----+\n|id |Name |year|store_id|Cost |\n+---+-----+----+--------+-----+\n|1  |Rahul|2018|10      |30000|\n|2  |Syam |2010|20      |40000|\n|3  |Mohan|2010|10      |40000|\n|4  |Mac  |2005|60      |35000|\n|5  |Tom  |2010|40      |38000|\n+---+-----+----+--------+-----+\n\n+--------+------+\n|Catagory|Cat_id|\n+--------+------+\n|Sports  |10    |\n|Books   |20    |\n|Women   |30    |\n|Men     |40    |\n+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "records = [(1,\"Rahul\",\"2018\",\"10\",30000), \n",
    "    (2,\"Syam\",\"2010\",\"20\",40000), \n",
    "    (3,\"Mohan\",\"2010\",\"10\",40000), \n",
    "    (4,\"Mac\",\"2005\",\"60\",35000), \n",
    "    (5,\"Tom\",\"2010\",\"40\",38000)]\n",
    "record_Columns = [\"id\",\"Name\",\"year\", \"store_id\",\"Cost\"]\n",
    "recordDF = spark.createDataFrame(data=records, schema = record_Columns)\n",
    "recordDF.show(truncate=False)\n",
    "\n",
    "store_master = [(\"Sports\",10),(\"Books\",20), (\"Women\",30), (\"Men\",40)]\n",
    "store_master_columns = [\"Catagory\",\"Cat_id\"]\n",
    "store_masterDF = spark.createDataFrame(data=store_master, schema = store_master_columns)\n",
    "store_masterDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db70f860-aa07-430c-b5fc-176983c271a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------+-----+\n| id|Name|year|store_id| Cost|\n+---+----+----+--------+-----+\n|  4| Mac|2005|      60|35000|\n+---+----+----+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "recordDF.join(store_masterDF, recordDF.store_id==store_masterDF.Cat_id , how='leftanti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17ea133a-f3e9-4fcd-9bb7-d6275366bf9e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "* **4.Question:Pyspark :Joining DataFrames in PySpark Without Duplicate Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7adcfb-23bb-4839-adf9-1ffa23d3d948",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+\n|  Name| ID|Gender|\n+------+---+------+\n|   Ram|  1|     M|\n|  Mike|  2|     M|\n|Rohini|  3|     M|\n| Maria|  4|     F|\n| Jenis|  5|     F|\n+------+---+------+\n\n+---+------+\n| ID|salary|\n+---+------+\n|  1|  3000|\n|  2|  4000|\n|  3|  4000|\n|  4|  4000|\n|  5|  1200|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(('Ram'),1,'M'),\n",
    "          (('Mike'),2,'M'),\n",
    "          (('Rohini'),3,'M'),\n",
    "          (('Maria'),4,'F'),\n",
    "          (('Jenis'),5,'F')]\n",
    "\n",
    "columns = [\"Name\",\"ID\",\"Gender\"]\n",
    " \n",
    "# Create the spark dataframe\n",
    "df1 = spark.createDataFrame(data=data, schema = columns)\n",
    " \n",
    "# Print the dataframe\n",
    "df1.show()\n",
    "\n",
    "\n",
    "# Create data in dataframe\n",
    "data2 = [(1,3000),\n",
    "          (2,4000),\n",
    "          (3,4000),\n",
    "          (4,4000),\n",
    "          (5, 1200)]\n",
    " \n",
    "# Column names in dataframe\n",
    "columns = [\"ID\",\"salary\"]\n",
    " \n",
    "# Create the spark dataframe\n",
    "df2 = spark.createDataFrame(data=data2,\n",
    "                            schema = columns)\n",
    " \n",
    "# Print the dataframe\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671da909-2652-4e91-9a8d-6385b2cb0176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+---+------+\n|  Name| ID|Gender| ID|salary|\n+------+---+------+---+------+\n|   Ram|  1|     M|  1|  3000|\n|  Mike|  2|     M|  2|  4000|\n|Rohini|  3|     M|  3|  4000|\n| Maria|  4|     F|  4|  4000|\n| Jenis|  5|     F|  5|  1200|\n+------+---+------+---+------+\n\n+---+------+------+------+\n| ID|  Name|Gender|salary|\n+---+------+------+------+\n|  1|   Ram|     M|  3000|\n|  2|  Mike|     M|  4000|\n|  3|Rohini|     M|  4000|\n|  4| Maria|     F|  4000|\n|  5| Jenis|     F|  1200|\n+---+------+------+------+\n\n+------+------+---+------+\n|  Name|Gender| ID|salary|\n+------+------+---+------+\n|   Ram|     M|  1|  3000|\n|  Mike|     M|  2|  4000|\n|Rohini|     M|  3|  4000|\n| Maria|     F|  4|  4000|\n| Jenis|     F|  5|  1200|\n+------+------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#join\n",
    "df = df1.join(df2, df1.ID==df2.ID)\n",
    "df.show()  #In result you can see Duplicate column of 'ID'. \n",
    "\n",
    "df_noDuplicates = df1.join(df2, ['ID']) #No duplicate column of 'ID'\n",
    "df_noDuplicates.show()\n",
    "\n",
    "df_another = df1.join(df2, df1.ID==df2.ID).drop(df1.ID).show()#No duplicate column of 'ID', as we are using 'drop' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9126d336-8d80-45c9-8704-1c72d8094096",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-189548932076915>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m df1\u001B[38;5;241m.\u001B[39mjoin(df2, df1\u001B[38;5;241m.\u001B[39mID\u001B[38;5;241m==\u001B[39mdf2\u001B[38;5;241m.\u001B[39mID)\u001B[38;5;241m.\u001B[39mdrop(\u001B[43mID\u001B[49m)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'ID' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-189548932076915>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m df1\u001B[38;5;241m.\u001B[39mjoin(df2, df1\u001B[38;5;241m.\u001B[39mID\u001B[38;5;241m==\u001B[39mdf2\u001B[38;5;241m.\u001B[39mID)\u001B[38;5;241m.\u001B[39mdrop(\u001B[43mID\u001B[49m)\u001B[38;5;241m.\u001B[39mshow()\n\n\u001B[0;31mNameError\u001B[0m: name 'ID' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'ID' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0edb76d2-acdf-49a1-9692-3c6f786057ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "practice pyspark intervoew question",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

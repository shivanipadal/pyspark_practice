{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4ee148fc-a307-4ed6-aa60-c4b62157a989","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bd4bcdfc-f804-4f5e-9595-fda13485beeb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8053920935473942#setting/sparkui/0403-094732-zivydkd3/driver-9095611367828609620\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8053920935473942#setting/sparkui/0403-094732-zivydkd3/driver-9095611367828609620\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n\ncolumns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n\nspark_df = spark.createDataFrame(data,schema=columns)\n\nspark_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"276fe388-24d4-44b0-8ddd-2dc87e57e919","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|  dob|gender|salary|\n+----------+-----------+---------+-----+------+------+\n|     James|           |    Smith|36636|     M| 60000|\n|   Michael|       Rose|         |40288|     M| 70000|\n|    Robert|           | Williams|42114|      |400000|\n|     Maria|       Anne|    Jones|39192|     F|500000|\n|       Jen|       Mary|    Brown|     |     F|     0|\n+----------+-----------+---------+-----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["spark_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e8e1735-b7a2-4af9-91d9-0c8d3bdaaab7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- first_name: string (nullable = true)\n |-- middle_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_pd=spark_df.toPandas()\ndf_pd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3cabf32b-ef91-439d-8206-3b781cfae551","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>first_name</th>\n      <th>middle_name</th>\n      <th>last_name</th>\n      <th>dob</th>\n      <th>gender</th>\n      <th>salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>James</td>\n      <td></td>\n      <td>Smith</td>\n      <td>36636</td>\n      <td>M</td>\n      <td>60000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Michael</td>\n      <td>Rose</td>\n      <td></td>\n      <td>40288</td>\n      <td>M</td>\n      <td>70000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Robert</td>\n      <td></td>\n      <td>Williams</td>\n      <td>42114</td>\n      <td></td>\n      <td>400000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Maria</td>\n      <td>Anne</td>\n      <td>Jones</td>\n      <td>39192</td>\n      <td>F</td>\n      <td>500000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Jen</td>\n      <td>Mary</td>\n      <td>Brown</td>\n      <td></td>\n      <td>F</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>first_name</th>\n      <th>middle_name</th>\n      <th>last_name</th>\n      <th>dob</th>\n      <th>gender</th>\n      <th>salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>James</td>\n      <td></td>\n      <td>Smith</td>\n      <td>36636</td>\n      <td>M</td>\n      <td>60000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Michael</td>\n      <td>Rose</td>\n      <td></td>\n      <td>40288</td>\n      <td>M</td>\n      <td>70000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Robert</td>\n      <td></td>\n      <td>Williams</td>\n      <td>42114</td>\n      <td></td>\n      <td>400000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Maria</td>\n      <td>Anne</td>\n      <td>Jones</td>\n      <td>39192</td>\n      <td>F</td>\n      <td>500000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Jen</td>\n      <td>Mary</td>\n      <td>Brown</td>\n      <td></td>\n      <td>F</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sspark_df=spark.createDataFrame(df_pd)\nsspark_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"343d32d4-3b33-452a-872a-916bdd43fdb7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----------+---------+-----+------+------+\n|first_name|middle_name|last_name|  dob|gender|salary|\n+----------+-----------+---------+-----+------+------+\n|     James|           |    Smith|36636|     M| 60000|\n|   Michael|       Rose|         |40288|     M| 70000|\n|    Robert|           | Williams|42114|      |400000|\n|     Maria|       Anne|    Jones|39192|     F|500000|\n|       Jen|       Mary|    Brown|     |     F|     0|\n+----------+-----------+---------+-----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data = [(\"1\", \"Be the change that you wish to see in the world\"),\n    (\"2\", \"Everyone thinks of changing the world, but no one thinks of changing himself.\"),\n    (\"3\", \"The purpose of our lives is to be happy.\"),\n    (\"4\", \"Be cool.\")]\ncolumns = [\"Seqno\",\"Quote\"]\ndf = spark.createDataFrame(data,columns)\ndf.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25795c35-4041-47c6-97ec-7860b3455dfe","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+--------------------+\n|Seqno|               Quote|\n+-----+--------------------+\n|    1|Be the change tha...|\n|    2|Everyone thinks o...|\n|    3|The purpose of ou...|\n|    4|            Be cool.|\n+-----+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8314851-c616-414a-b614-35c1b7bcb87f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----------------------------------------------------------------------------+\n|Seqno|Quote                                                                        |\n+-----+-----------------------------------------------------------------------------+\n|1    |Be the change that you wish to see in the world                              |\n|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n|3    |The purpose of our lives is to be happy.                                     |\n|4    |Be cool.                                                                     |\n+-----+-----------------------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.show(2, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"71541736-0906-4f79-8c01-7eb5236f1e91","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----------------------------------------------------------------------------+\n|Seqno|Quote                                                                        |\n+-----+-----------------------------------------------------------------------------+\n|1    |Be the change that you wish to see in the world                              |\n|2    |Everyone thinks of changing the world, but no one thinks of changing himself.|\n+-----+-----------------------------------------------------------------------------+\nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.show(vertical=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"efa813ec-d159-4e44-b887-066e03fc1645","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["-RECORD 0---------------------\n Seqno | 1                    \n Quote | Be the change tha... \n-RECORD 1---------------------\n Seqno | 2                    \n Quote | Everyone thinks o... \n-RECORD 2---------------------\n Seqno | 3                    \n Quote | The purpose of ou... \n-RECORD 3---------------------\n Seqno | 4                    \n Quote | Be cool.             \n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf.select(col('Seqno')).show()\n\ndf.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5c37581d-26b1-4d11-a16c-62edb8855683","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+\n|Seqno|\n+-----+\n|    1|\n|    2|\n|    3|\n|    4|\n+-----+\n\nOut[27]: [Row(Seqno='1', Quote='Be the change that you wish to see in the world'),\n Row(Seqno='2', Quote='Everyone thinks of changing the world, but no one thinks of changing himself.'),\n Row(Seqno='3', Quote='The purpose of our lives is to be happy.'),\n Row(Seqno='4', Quote='Be cool.')]"]}],"execution_count":0},{"cell_type":"code","source":["# df.show()\n\nfrom pyspark.sql import functions as F\n\n# df=df.withColumn('New_col', F.lit('New_column'))\ndf=df.withColumnRenamed('New_col', 'LOL1').show()\ndf.drop(\"LOL1\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a215b085-2cc6-453b-bb66-62daf44548a8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-663903994903831>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfunctions\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'New_col'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'New_column'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'New_col'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'LOL1'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"LOL1\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'withColumn'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'withColumn'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-663903994903831>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfunctions\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'New_col'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'New_column'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwithColumnRenamed\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'New_col'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'LOL1'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"LOL1\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'withColumn'"]}}],"execution_count":0},{"cell_type":"code","source":["data2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),\n     (4,\"Rames Rose\"),(5,\"Rames rose\")\n  ]\ndf2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\ndf2.filter(col('name').like('%rose%')).show()\n\n# \"(?i)^*rose$\")\ndf2.filter(col('name').rlike(\"(?i)^*rose$\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e4f437b-0c17-41bd-8508-b1a3e84893b4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+\n| id|      name|\n+---+----------+\n|  5|Rames rose|\n+---+----------+\n\n+---+------------+\n| id|        name|\n+---+------------+\n|  2|Michael Rose|\n|  4|  Rames Rose|\n|  5|  Rames rose|\n+---+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Prepare Data\ndata = [(\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600), \\\n    (\"Robert\", \"Sales\", 4100), \\\n    (\"Maria\", \"Finance\", 3000), \\\n    (\"James\", \"Sales\", 3000), \\\n    (\"Scott\", \"Finance\", 3300), \\\n    (\"Jen\", \"Finance\", 3900), \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000), \\\n    (\"Saif\", \"Sales\", 4100) \\\n  ]\n\n# Create DataFrame\ncolumns= [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"abc7dc51-badc-4b59-a93a-4a217f7d5ad2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["distinctDF=df.distinct()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1ab13fc-1596-44e4-9d6d-62df94ac2134","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(\"Distinct count: \"+str(distinctDF.count()))\ndistinctDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"44c20814-2ebe-43d4-a9b7-82d3fa6bc2e2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Distinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n  ]\ncolumns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5f6b2731-0437-4dde-b3c2-e8359feabff2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.orderBy(col('salary').asc()).orderBy(col('department').desc()).show()\n\ndf.sort(df.department.asc(),df.state.asc()).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a61b62b2-1ec4-46aa-96ea-6eda2b1de40f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n+-------------+----------+-----+------+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.createOrReplaceTempView('Sort_table')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d298f57-e2b0-49ce-b1af-32c1f0058be4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["val=spark.sql(\"\"\"\nselect * from sort_table\norer by salary,department\n\"\"\")\n\nval.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0e6b382-83e3-4884-bed8-e5c628a5e31e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-663903994903839>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m val=spark.sql(\"\"\"\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mselect\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msort_table\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0morer\u001B[0m \u001B[0mby\u001B[0m \u001B[0msalary\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdepartment\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \"\"\")\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36msql\u001B[0;34m(self, sqlQuery, **kwargs)\u001B[0m\n\u001B[1;32m   1117\u001B[0m             \u001B[0msqlQuery\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformatter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1118\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1119\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1120\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1121\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mParseException\u001B[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'by'(line 3, pos 5)\n\n== SQL ==\n\nselect * from sort_table\norer by salary,department\n-----^^^\n","errorSummary":"<span class='ansi-red-fg'>ParseException</span>: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'by'(line 3, pos 5)\n\n== SQL ==\n\nselect * from sort_table\norer by salary,department\n-----^^^\n","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-663903994903839>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m val=spark.sql(\"\"\"\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mselect\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msort_table\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0morer\u001B[0m \u001B[0mby\u001B[0m \u001B[0msalary\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdepartment\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \"\"\")\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/session.py\u001B[0m in \u001B[0;36msql\u001B[0;34m(self, sqlQuery, **kwargs)\u001B[0m\n\u001B[1;32m   1117\u001B[0m             \u001B[0msqlQuery\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformatter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1118\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1119\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msqlQuery\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1120\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1121\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mParseException\u001B[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'by'(line 3, pos 5)\n\n== SQL ==\n\nselect * from sort_table\norer by salary,department\n-----^^^\n"]}}],"execution_count":0},{"cell_type":"code","source":["simpleData = ((\"Java\",4000,5), \\\n    (\"Python\", 4600,10),  \\\n    (\"Scala\", 4100,15),   \\\n    (\"Scala\", 4500,15),   \\\n    (\"PHP\", 3000,20),  \\\n  )\ncolumns= [\"CourseName\", \"fee\", \"discount\"]\n\n# Create DataFrame\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"66a33fd8-f31b-44ac-b64f-155293c04e30","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- CourseName: string (nullable = true)\n |-- fee: long (nullable = true)\n |-- discount: long (nullable = true)\n\n+----------+----+--------+\n|CourseName|fee |discount|\n+----------+----+--------+\n|Java      |4000|5       |\n|Python    |4600|10      |\n|Scala     |4100|15      |\n|Scala     |4500|15      |\n|PHP       |3000|20      |\n+----------+----+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import upper\ndef to_upper_str_column(df):\n    return df.withColumn('CourseName',upper(df.CourseName))\n\ndef reduce_price(df, reduceBy):\n    return df.withColumn(\"new_fee\", df.fee - reduceBy )\n\ndef apply_discount(df):\n    return df.withColumn('disc_fee', df.new_fee -(df.new_fee * df.discount)/100)\n\n\ndf2=df.transform(to_upper_str_column) \\\n       .transform(reduce_price,1000) \\\n       .transform(apply_discount)\n\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98acd1fb-20c6-4567-a5aa-c7e73c7b5932","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+----+--------+-------+--------+\n|CourseName| fee|discount|new_fee|disc_fee|\n+----------+----+--------+-------+--------+\n|      JAVA|4000|       5|   3000|  2850.0|\n|    PYTHON|4600|      10|   3600|  3240.0|\n|     SCALA|4100|      15|   3100|  2635.0|\n|     SCALA|4500|      15|   3500|  2975.0|\n|       PHP|3000|      20|   2000|  1600.0|\n+----------+----+--------+-------+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Pyspark pandas apply\n\nimport pyspark.pandas as ps\nimport numpy as np\n\ntech = ({'Fee':[20000,25000,30000,22000,np.NaN],\n        'Discount':[1000,2500,1500,1200,3000]\n        })\n\npsdf = ps.DataFrame(tech)\n\nprint(psdf)\n\ndef add(data):\n    return data[0]+ data[1]\n\naddDF = psdf.apply(add, axis=1)\n\nprint(addDF)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04aaca1c-fc87-457b-880b-25e30541d184","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["       Fee  Discount\n0  20000.0      1000\n1  25000.0      2500\n2  30000.0      1500\n3  22000.0      1200\n4      NaN      3000\n0    21000.0\n1    27500.0\n2    31500.0\n3    23200.0\n4        NaN\ndtype: float64\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n\n@udf(returnType=StringType())\ndef convertCase(str):\n    return str.upper()\n\n\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n  ]\ncolumns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\n# df.printSchema()\n#df.show(truncate=False)\n\ndf = df.withColumn(\"Newcol_upper\",convertCase(col('employee_name')))\n\ndf.show()\n\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53374012-c129-425f-9943-a2f65c3766e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+-----+------+---+-----+------------+\n|employee_name|department|state|salary|age|bonus|Newcol_upper|\n+-------------+----------+-----+------+---+-----+------------+\n|        James|     Sales|   NY| 90000| 34|10000|       JAMES|\n|      Michael|     Sales|   NY| 86000| 56|20000|     MICHAEL|\n|       Robert|     Sales|   CA| 81000| 30|23000|      ROBERT|\n|        Maria|   Finance|   CA| 90000| 24|23000|       MARIA|\n|        Raman|   Finance|   CA| 99000| 40|24000|       RAMAN|\n|        Scott|   Finance|   NY| 83000| 36|19000|       SCOTT|\n|          Jen|   Finance|   NY| 79000| 53|15000|         JEN|\n|         Jeff| Marketing|   CA| 80000| 25|18000|        JEFF|\n|        Kumar| Marketing|   NY| 91000| 50|21000|       KUMAR|\n+-------------+----------+-----+------+---+-----+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Import\nfrom pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\n# Prepare Data\ncolumns = [\"Seqno\",\"Name\"]\ndata = [(\"1\", \"john jones\"),\n    (\"2\", \"tracey smith\"),\n    (\"3\", \"amy sanders\")]\n\n# Create DataFrame\ndf = spark.createDataFrame(data=data,schema=columns)\n#df.show()\n\n# foreach() Example\ndef f(df):\n    print(df.Seqno)\ndf.foreach(f)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21ecb93c-7989-4cf8-b71d-3cda78924705","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\n#Create spark session\ndata = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n\ncolumns= [\"Product\",\"Amount\",\"Country\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a157bb9b-c41f-404d-9d46-db4e17ceba6a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- Product: string (nullable = true)\n |-- Amount: long (nullable = true)\n |-- Country: string (nullable = true)\n\n+-------+------+-------+\n|Product|Amount|Country|\n+-------+------+-------+\n|Banana |1000  |USA    |\n|Carrots|1500  |USA    |\n|Beans  |1600  |USA    |\n|Orange |2000  |USA    |\n|Orange |2000  |USA    |\n|Banana |400   |China  |\n|Carrots|1200  |China  |\n|Beans  |1500  |China  |\n|Orange |4000  |China  |\n|Banana |2000  |Canada |\n|Carrots|2000  |Canada |\n|Beans  |2000  |Mexico |\n+-------+------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_pivot = df.groupBy('Product').pivot('Country').sum('Amount')\ndf_pivot.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b782fb13-f4e0-48cf-96a4-f850abea13f4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------+-----+------+----+\n|Product|Canada|China|Mexico| USA|\n+-------+------+-----+------+----+\n| Orange|  null| 4000|  null|4000|\n|  Beans|  null| 1500|  2000|1600|\n| Banana|  2000|  400|  null|1000|\n|Carrots|  2000| 1200|  null|1500|\n+-------+------+-----+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import expr\n\nunpivotExpr=\"stack(3,'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country, Total)\"\nunPivotDF = df_pivot.select('Product', expr(unpivotExpr)).where('Total is not null')\nunPivotDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"09ae39b0-9247-4ed8-8a71-eeea64543907","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-------+-----+\n|Product|Country|Total|\n+-------+-------+-----+\n| Orange|  China| 4000|\n|  Beans|  China| 1500|\n|  Beans| Mexico| 2000|\n| Banana| Canada| 2000|\n| Banana|  China|  400|\n|Carrots| Canada| 2000|\n|Carrots|  China| 1200|\n+-------+-------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_pivot=df.groupBy('Country').pivot('Product').sum('Amount')\n\ndf_pivot.show()\n\nunpivot_expr = \"stack(4, 'Banana', Banana, 'Beans', Beans, 'Carrots', Carrots, 'Orange', Orange) as (Product, Total)\"\nunPivotDF =df_pivot.select('Country', expr(unpivot_expr)).where('Total is not null')\n\nunPivotDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0143cc2a-2550-4bb6-9b4f-46d2d3628c45","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------+-----+-------+------+\n|Country|Banana|Beans|Carrots|Orange|\n+-------+------+-----+-------+------+\n|  China|   400| 1500|   1200|  4000|\n|    USA|  1000| 1600|   1500|  4000|\n| Mexico|  null| 2000|   null|  null|\n| Canada|  2000| null|   2000|  null|\n+-------+------+-----+-------+------+\n\n+-------+-------+-----+\n|Country|Product|Total|\n+-------+-------+-----+\n|  China| Banana|  400|\n|  China|  Beans| 1500|\n|  China|Carrots| 1200|\n|  China| Orange| 4000|\n|    USA| Banana| 1000|\n|    USA|  Beans| 1600|\n|    USA|Carrots| 1500|\n|    USA| Orange| 4000|\n| Mexico|  Beans| 2000|\n| Canada| Banana| 2000|\n| Canada|Carrots| 2000|\n+-------+-------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number \n\nspark = SparkSession.builder.master(\"local[*]\").appName('Test').getOrCreate()\n\n\n# spark = SparkSession.builder \\\n#     .master(\"local[*]\") \\\n#     .appName('test') \\\n#     .getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e9fe9908-e067-4c52-a8df-adb9615cfab0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"88eb1194-5ffe-480e-a7dc-aaecf688db95","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8053920935473942#setting/sparkui/0405-084652-olhkqi7v/driver-2351774885519977753\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=8053920935473942#setting/sparkui/0405-084652-olhkqi7v/driver-2351774885519977753\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["simpleData = ((\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600),  \\\n    (\"Robert\", \"Sales\", 4100),   \\\n    (\"Maria\", \"Finance\", 3000),  \\\n    (\"James\", \"Sales\", 3000),    \\\n    (\"Scott\", \"Finance\", 3300),  \\\n    (\"Jen\", \"Finance\", 3900),    \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000),\\\n    (\"Saif\", \"Sales\", 4100) \\\n  )\n \ncolumns= [\"employee_name\", \"department\", \"salary\"]\n\ndf = spark.createDataFrame(simpleData,schema=columns)\n\nwindow_expr = Window.partitionBy('department').orderBy('salary')\n\ndf.withColumn('row_num', row_number().over(window_expr)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ddbff5e9-cea2-4847-a369-a01e66db6c92","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+-------+\n|employee_name|department|salary|row_num|\n+-------------+----------+------+-------+\n|        Maria|   Finance|  3000|      1|\n|        Scott|   Finance|  3300|      2|\n|          Jen|   Finance|  3900|      3|\n|        Kumar| Marketing|  2000|      1|\n|         Jeff| Marketing|  3000|      2|\n|        James|     Sales|  3000|      1|\n|        James|     Sales|  3000|      2|\n|       Robert|     Sales|  4100|      3|\n|         Saif|     Sales|  4100|      4|\n|      Michael|     Sales|  4600|      5|\n+-------------+----------+------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["win_expr = Window.partitionBy('department').orderBy('salary')\n\ndf.withColumn('rank', row_number().over(win_expr)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c785ee9-e6a2-46ed-9ba4-45e0751037e8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|   1|\n|        Scott|   Finance|  3300|   2|\n|          Jen|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|         Jeff| Marketing|  3000|   2|\n|        James|     Sales|  3000|   1|\n|        James|     Sales|  3000|   2|\n|       Robert|     Sales|  4100|   3|\n|         Saif|     Sales|  4100|   4|\n|      Michael|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import rank,dense_rank\nwindow_exp = Window.partitionBy('department').orderBy('salary')\n\ndf.withColumn('rank', rank().over(window_expr)).show()\ndf.withColumn('dense_rank', dense_rank().over(window_exp)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5c479e1d-109b-45a3-86c7-763aaad12024","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|   1|\n|        Scott|   Finance|  3300|   2|\n|          Jen|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|         Jeff| Marketing|  3000|   2|\n|        James|     Sales|  3000|   1|\n|        James|     Sales|  3000|   1|\n|       Robert|     Sales|  4100|   3|\n|         Saif|     Sales|  4100|   3|\n|      Michael|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n+-------------+----------+------+----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\nwindowSpecAgg  = Window.partitionBy(\"department\")\nfrom pyspark.sql.functions import col,avg,sum,min,max,row_number \ndf.withColumn(\"row\",row_number().over(windowSpec)) \\\n  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n  .show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e836e542-e12a-464b-a2e3-3fccd1889601","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------+-----+----+----+\n|department|   avg|  sum| min| max|\n+----------+------+-----+----+----+\n|   Finance|3400.0|10200|3000|3900|\n| Marketing|2500.0| 5000|2000|3000|\n|     Sales|3760.0|18800|3000|4600|\n+----------+------+-----+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select('department', col('salary').alias('sal')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"02de9325-62f3-4673-92e8-f1f2c6840819","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+----+\n|department| sal|\n+----------+----+\n|     Sales|3000|\n|     Sales|4600|\n|     Sales|4100|\n|   Finance|3000|\n|     Sales|3000|\n|   Finance|3300|\n|   Finance|3900|\n| Marketing|3000|\n| Marketing|2000|\n|     Sales|4100|\n+----------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2de1277-00e6-4b00-9e14-9505563f7f29","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"practice","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":84784760545351}},"nbformat":4,"nbformat_minor":0}

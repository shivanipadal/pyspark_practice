{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark=SparkSession.builder.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39231566-25a7-46ab-94fb-02e7957fdbb6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Create DF using list of rows\nfrom datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf=spark.createDataFrame([Row(a=1,b=2.,c='string1',d=date(2000,1,1),e=datetime(2000,1,1,12,0)),\n                         Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n                         Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n                         ])\ndf.show()\ndf.printSchema()\ndf\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"472944e6-016e-49fd-9ef2-5a45146da610","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"},"removedWidgets":[],"addedWidgets":{},"executionCount":null,"metadata":{"kernelSessionId":"d58182d8-8b04b774cc1e124b2ab437d3"},"type":"mimeBundle","arguments":{}}},"output_type":"display_data","data":{"text/plain":"","application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n+---+---+-------+----------+-------------------+\n\nroot\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\nOut[3]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]}],"execution_count":0},{"cell_type":"code","source":["#Create a PySpark DataFrame with an explicit schema.\ndf = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., '%storering2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\ndf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23375cd5-d885-490c-bd8b-f7a09f0c11c1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[4]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]}],"execution_count":0},{"cell_type":"code","source":["#Create a PySpark DataFrame from a pandas DataFrame\npd_df=pd.DataFrame({'a':[1,2,3],'b':[2.,3.,4],'c':['string1','string2','string3'],'d':[date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],'e':[datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]})\n\ndf=spark.createDataFrame(pd_df)\ndf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48488fb9-7a15-4f53-ab05-bf90db4256ac","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[5]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]}],"execution_count":0},{"cell_type":"code","source":["#Create a PySpark DataFrame from an RDD consisting of a list of tuples.\nrdd=spark.sparkContext.parallelize([(1,2.,'string1',date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n                                 (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n                                 (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))])\ndf=spark.createDataFrame(rdd,schema=['a','b','c','d','e'])\ndf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5cdb3fd-183d-4358-bd6e-3f96caf70793","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[6]: DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]}],"execution_count":0},{"cell_type":"code","source":["# All DataFrames above result same.\ndf.show()\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dcb79a1b-ccd2-4692-a2dd-3c2fed9b0fac","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n+---+---+-------+----------+-------------------+\n\nroot\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\ndf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe522867-12ad-4193-ad66-e8cb930ed2b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<table border='1'>\n<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n</table>\n","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<table border='1'>\n<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n</table>\n"]}}],"execution_count":0},{"cell_type":"code","source":["df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7eec996f-f0b0-466c-a152-f02abb8258b1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+-------+----------+-------------------+\n|a  |b  |c      |d         |e                  |\n+---+---+-------+----------+-------------------+\n|1  |2.0|string1|2000-01-01|2000-01-01 12:00:00|\n|2  |3.0|string2|2000-02-01|2000-01-02 12:00:00|\n|3  |4.0|string3|2000-03-01|2000-01-03 12:00:00|\n+---+---+-------+----------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[" input_data = [(\"Mobile(Fluid Black, 8GB RAM, 128GB Storage)\",\n                   112345, 4.0, 12499),\n                   \n                  (\"LED TV\", 114567, 4.2, 49999),\n                   \n                  (\"Refrigerator\", 123543, 4.4, 13899),\n                   \n                  (\"6.5 kg Fully-Automatic Top Loading Washing Machine \\\n                  (WA65A4002VS/TL, Imperial Silver, Center Jet Technology)\",\n                   113465, 3.9, 6999),\n                   \n                  (\"T-shirt\", 124378, 4.1, 1999),\n                   \n                  (\"Jeans\", 126754, 3.7, 3999),\n                   \n                  (\"Men's Casual Shoes in White Sneakers for Outdoor and\\\n                  Daily use\", 134565, 4.7, 1499),\n                   \n                  (\"Vitamin C Ultra Light Gel Oil-Free Moisturizer\",\n                   145234, 4.6, 999),\n                  ]\n \n schema = [\"Name\", \"ID\", \"Rating\", \"Price\"]\n     \n # calling function to create dataframe\n df = spark.createDataFrame(input_data, schema)\n df.show()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"63566296-6346-4dd5-8e55-52a8be212d6f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+------+------+-----+\n|                Name|    ID|Rating|Price|\n+--------------------+------+------+-----+\n|Mobile(Fluid Blac...|112345|   4.0|12499|\n|              LED TV|114567|   4.2|49999|\n|        Refrigerator|123543|   4.4|13899|\n|6.5 kg Fully-Auto...|113465|   3.9| 6999|\n|             T-shirt|124378|   4.1| 1999|\n|               Jeans|126754|   3.7| 3999|\n|Men's Casual Shoe...|134565|   4.7| 1499|\n|Vitamin C Ultra L...|145234|   4.6|  999|\n+--------------------+------+------+-----+\n\n+----------------------------------------------------------------------------------------------------------------------------+------+------+-----+\n|Name                                                                                                                        |ID    |Rating|Price|\n+----------------------------------------------------------------------------------------------------------------------------+------+------+-----+\n|Mobile(Fluid Black, 8GB RAM, 128GB Storage)                                                                                 |112345|4.0   |12499|\n|LED TV                                                                                                                      |114567|4.2   |49999|\n|Refrigerator                                                                                                                |123543|4.4   |13899|\n|6.5 kg Fully-Automatic Top Loading Washing Machine                  (WA65A4002VS/TL, Imperial Silver, Center Jet Technology)|113465|3.9   |6999 |\n|T-shirt                                                                                                                     |124378|4.1   |1999 |\n|Jeans                                                                                                                       |126754|3.7   |3999 |\n|Men's Casual Shoes in White Sneakers for Outdoor and                 Daily use                                              |134565|4.7   |1499 |\n|Vitamin C Ultra Light Gel Oil-Free Moisturizer                                                                              |145234|4.6   |999  |\n+----------------------------------------------------------------------------------------------------------------------------+------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Create a PySpark DataFrame with an explicit schema.\ndf = spark.createDataFrame([\n    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n    (2, 3., '%storering2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n], schema='a long, b double, c string, d date, e timestamp')\ndf\ndf.show(2,vertical=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"241f3526-78ce-44ca-ad15-2b3ebe539cc8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["-RECORD 0------------------\n a   | 1                   \n b   | 2.0                 \n c   | string1             \n d   | 2000-01-01          \n e   | 2000-01-01 12:00:00 \n-RECORD 1------------------\n a   | 2                   \n b   | 3.0                 \n c   | %storering2         \n d   | 2000-02-01          \n e   | 2000-01-02 12:00:00 \nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#You can see the DataFrame's schema and column names as follows:\nprint(df.columns)\nprint(df.printSchema())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aac64df3-3936-43d6-ba28-9bd77f4eaf14","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["['a', 'b', 'c', 'd', 'e']\nroot\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\nNone\n"]}],"execution_count":0},{"cell_type":"code","source":["#Show the summary of the DataFrame\ndf.select('a','b','c').describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75a0eaf7-6713-4e11-8a01-d1ef80d542fd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+---+---+-----------+\n|summary|  a|  b|          c|\n+-------+---+---+-----------+\n|  count|  3|  3|          3|\n|   mean|2.0|3.0|       null|\n| stddev|1.0|1.0|       null|\n|    min|  1|2.0|%storering2|\n|    max|  3|4.0|    string3|\n+-------+---+---+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.collect() #Chance of getting out of memory incase of huge data, to avoid this erroe use df.take() or df.tail() "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a695da09-1bd1-4c12-84c1-887d8eb11eb9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[14]: [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n Row(a=2, b=3.0, c='%storering2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"]}],"execution_count":0},{"cell_type":"code","source":["df.take(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6428b9ff-8ac2-4199-bcb9-df4fd8f267d7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[15]: [Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n Row(a=2, b=3.0, c='%storering2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0))]"]}],"execution_count":0},{"cell_type":"code","source":["df.tail(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e0c147b-3f54-4367-8021-f9b0eb08cf5d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[16]: [Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"]}],"execution_count":0},{"cell_type":"code","source":["df.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af3cdd88-7b69-4588-8ca5-3a8fae6b8811","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th>d</th>\n      <th>e</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2.0</td>\n      <td>string1</td>\n      <td>2000-01-01</td>\n      <td>2000-01-01 12:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3.0</td>\n      <td>%storering2</td>\n      <td>2000-02-01</td>\n      <td>2000-01-02 12:00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.0</td>\n      <td>string3</td>\n      <td>2000-03-01</td>\n      <td>2000-01-03 12:00:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>b</th>\n      <th>c</th>\n      <th>d</th>\n      <th>e</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2.0</td>\n      <td>string1</td>\n      <td>2000-01-01</td>\n      <td>2000-01-01 12:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3.0</td>\n      <td>%storering2</td>\n      <td>2000-02-01</td>\n      <td>2000-01-02 12:00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>4.0</td>\n      <td>string3</td>\n      <td>2000-03-01</td>\n      <td>2000-01-03 12:00:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.a"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98791bb7-9429-4756-8946-ddb9e8778302","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[18]: Column<'a'>"]}],"execution_count":0},{"cell_type":"code","source":["df.select('c').show()\ndf.select(df.c).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7eed6d3f-3e9d-4c44-92b8-734db3ff90ca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------+\n|          c|\n+-----------+\n|    string1|\n|%storering2|\n|    string3|\n+-----------+\n\n+-----------+\n|          c|\n+-----------+\n|    string1|\n|%storering2|\n|    string3|\n+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn('Upper_C',df.c).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"322ad359-2ec0-47d1-a737-635872e8ab89","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+-----------+----------+-------------------+-----------+\n|  a|  b|          c|         d|                  e|    Upper_C|\n+---+---+-----------+----------+-------------------+-----------+\n|  1|2.0|    string1|2000-01-01|2000-01-01 12:00:00|    string1|\n|  2|3.0|%storering2|2000-02-01|2000-01-02 12:00:00|%storering2|\n|  3|4.0|    string3|2000-03-01|2000-01-03 12:00:00|    string3|\n+---+---+-----------+----------+-------------------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#filter\n\ndf.filter(df.a == 1).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c323830c-d3d5-42f7-9a63-b5aa72a01975","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Applying functions\n\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series :\n    return series+1\n\ndf.select(pandas_plus_one(df.a)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"837546b9-adb1-40c2-b879-13b9a5f29fea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------+\n|pandas_plus_one(a)|\n+------------------+\n|                 2|\n|                 3|\n|                 4|\n+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#DataFrame.mapInPandas which allows users directly use the APIs in a pandas DataFrame without any restrictions such as the result length.\n\ndef pandas_filter_func(iterator):\n    for pandas_df in iterator:\n        yield pandas_df[pandas_df.a == 1]\n        \ndf.mapInPandas(pandas_filter_func,schema=df.schema).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e5a3ab4a-3e33-46e9-b912-f08c855080dd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+-------+----------+-------------------+\n|  a|  b|      c|         d|                  e|\n+---+---+-------+----------+-------------------+\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n+---+---+-------+----------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame([\n    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0eba64a-8cce-432b-b0cf-71f38809ce7e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------+---+---+\n|color| fruit| v1| v2|\n+-----+------+---+---+\n|  red|banana|  1| 10|\n| blue|banana|  2| 20|\n|  red|carrot|  3| 30|\n| blue| grape|  4| 40|\n|  red|carrot|  5| 50|\n|black|carrot|  6| 60|\n|  red|banana|  7| 70|\n|  red| grape|  8| 80|\n+-----+------+---+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.groupBy('color').avg().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"95a4c460-0c37-4444-a215-506781ec32a7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-------+-------+\n|color|avg(v1)|avg(v2)|\n+-----+-------+-------+\n|  red|    4.8|   48.0|\n| blue|    3.0|   30.0|\n|black|    6.0|   60.0|\n+-----+-------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["def plus_mean(pandas_df):\n    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n\ndf.groupby('color').applyInPandas(plus_mean,schema=df.schema).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"476d147d-163c-4743-9d40-ce7fb356e8e6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------+---+---+\n|color| fruit| v1| v2|\n+-----+------+---+---+\n|black|carrot|  0| 60|\n| blue|banana| -1| 20|\n| blue| grape|  1| 40|\n|  red|banana| -3| 10|\n|  red|carrot| -1| 30|\n|  red|carrot|  0| 50|\n|  red|banana|  2| 70|\n|  red| grape|  3| 80|\n+-----+------+---+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.write.csv('head.csv',header=True,mode='overwrite')\ns#park.read.csv('head.csv', header=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"011f40ab-43a6-4cc3-9f24-8bd2425d24ea","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2424063623817665>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'head.csv'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'overwrite'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0ms\u001B[0m\u001B[0;31m#park.read.csv('head.csv', header=True)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 's' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 's' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-2424063623817665>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcsv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'head.csv'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mheader\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'overwrite'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0ms\u001B[0m\u001B[0;31m#park.read.csv('head.csv', header=True)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 's' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["#Working with SQL\ndf.createOrReplaceTempView('tableA')\nspark.sql('select count(*) from tableA')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d89e8fc-bca0-455f-a502-c54dc602da05","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql('select * from tableA')\nspark.sql('select color,  max(v1) from tableA group by color')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b608ee9-a852-427f-bccc-de2a6af2b298","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["###Quickstart: Pandas API on Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0d0b787a-9715-4ec8-944e-d18ff714eee5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np \nimport pyspark.pandas as ps\nfrom pyspark.sql import SparkSession\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dfc77ecd-317d-4eb1-85c4-430466616368","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["s=ps.Series([1,2,3,np.nan,6,8])\ns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db378bc0-26d8-4c20-afe6-09c7ff7169b0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#Creating a pandas-on-Spark DataFrame by passing a dict of objects that can be converted to series-like\npsdf=ps.DataFrame({'a':[1,2,3,4,5],\n                  'b':[100,200,300,400,500],\n                  'c':['one','two','three','four','five'],\n                  },index=[10,20,30,40,50])\n\nprint(psdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1bf38bf8-570a-41ec-b5b3-6c80fefb73d3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["dates=pd.date_range('20220101',freq='D',periods=6)\ndates"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4444e971-c5c4-4193-a3c7-22f20d98ae12","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["pd_df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))\nprint(pd_df)\ndf_ps = ps.from_pandas(pd_df)\ntype(df_ps)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cf696671-462a-45be-9a61-caca10a3db39","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf=spark.createDataFrame(pd_df)\npsdf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9d64274-abfe-4113-9734-c7d41f8dfcc8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#Creating pandas-on-Spark DataFrame from Spark DataFrame.\n\npsdf = psdf.pandas_api()\nprint(psdf)\ntype(psdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"31ab9592-e403-4334-a28d-c04f96464c33","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0bfc6da5-f0e5-4bc0-87b0-40cd905b0f9b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"17d0fe98-2e4f-42b7-9514-8036b21153ca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["print(psdf.index)\nprint(psdf.columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c1a9ea3f-8537-4943-a352-b1030b7ef23f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf.to_numpy()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7c8acce0-c88b-44c0-9e3e-78cf556d485d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf.describe()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"06a84faa-5842-4a40-aff1-ce92f9b311e9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf.T"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"22997e19-4a20-4f19-94a7-db01b8625c0b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf.sort_index(ascending=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2b92cb1d-c3e9-4813-9d67-c0be24aa4ba0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf.sort_values(by='B')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15c00fb9-e5fe-4687-8ab2-153048496cb4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["###Missing Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6150a1c2-7b5e-4407-b7c2-32e4e834f745","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np \nimport pyspark.pandas as ps\nfrom pyspark.sql import SparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d642639-dc41-483f-ba3b-724251c312ff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["dates = pd.date_range('20130101', periods=6)\npdf=pd.DataFrame(np.random.randn(6,4), index=dates , columns=list('ABCD'))\npdf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"892c40d2-d677-430a-95d9-f4763b9aec34","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["pdf1=pdf.reindex(index=dates[0:4],columns=list(pdf.columns)+['E'])\npdf1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5c361156-6450-4b39-a020-a808fff3c592","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["pdf1.loc[dates[0]:dates[1],'E'] = 1\npdf1\ntype(pdf1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"315d1ad5-31fd-49ec-bda2-7007248304d6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf1=ps.from_pandas(pdf1)\npsdf1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bdaf7bb5-dff4-4ceb-9de9-e18932a42ba1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#Drop any row that having missing data\npsdf1.dropna(how='any')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5300f2a4-d16a-4cdb-81d9-0efea08d7b09","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#fill na \npsdf1.fillna('Missing Value')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c6ac9528-67ce-4300-a197-e1c848673157","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["###Operation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8b2b76c6-677a-42b0-9efa-bc0916118dd7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Stats"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46166d1a-ceff-4a15-bba1-d6eb139ef4a2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["psdf1.mean()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"93daa2d9-0eeb-40f2-9426-bcb9d3945de4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Spark Configurations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b09136a7-3364-46b7-9353-c2494f71ecb7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["prev = spark.conf.get('spark.sql.execution.arrow.pyspark.enabled') #Keep it's efault value\nps.set_option('compute.default_index_type', 'distributed')  #Use default index prevent overhead\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore warnings coming from Arrow optimizations\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e7c9765-1b66-4f6d-9b9a-db3979103f45","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)\n%timeit ps.range(300000).to_pandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f08ad9ac-b8c6-4c26-b330-cc8138a4704e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', False)\n%timeit ps.range(300000).to_pandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e35c589c-8021-4bb9-953d-50c94c21f61a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["ps.reset_option('compute.default_index_type')\nspark.conf.set('spark.sql.execution.arrow.pyspark.enabled', prev)  #Set it's default va;ue back "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"55381ea5-f09e-4046-9b29-26444d8cede2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["##Grouping"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e1b6edb-d9da-4629-b8bd-24b944eb89d4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["psdf = ps.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n                          'foo', 'bar', 'foo', 'foo'],\n                    'B': ['one', 'one', 'two', 'three',\n                          'two', 'two', 'one', 'three'],\n                    'C': np.random.randn(8),\n                    'D': np.random.randn(8)})\npsdf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90f121f8-7506-4198-a226-62d9cfe81cf8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf.groupby('A').sum()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39c57215-6fec-44a9-aaf3-455213657539","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["psdf.groupby(['A','B']).sum()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0256c00b-d5b5-4d70-98a3-9adb58eee03e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["## Plotting"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"528c398e-def5-4e36-8a7b-cac47ec2bcff","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["pser=pd.Series(np.random.randn(10),index=pd.date_range('1/1/2022',periods=10))\n#print(pser)\npsser=ps.Series(pser)\n#print(psser)\npsser=psser.cummax()\nprint(psser)\npsser.plot()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8674e8e4-1e2c-4a02-af52-aaabd6c330b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["##Apache Arrow in PySpark\n(https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78be54d9-fc4d-4464-a52e-db1fecef6c22","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark.conf.set('spark.sql.execute.arrow.pyspark.enabled', True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d7b7761d-1737-4f89-9804-58c29fb9ac8c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["pdf=pd.DataFrame(np.random.rand(100,3))\npdf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d397236f-359b-48f9-a832-e46b7deb67bb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["df=spark.createDataFrame(pdf)\ntype(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"389b5843-763e-4036-ad5c-189a0a6950a4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["sdf=df.select('*').toPandas()\ntype(sdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89158215-61d4-493c-974f-7a56feff7b37","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["###Pandas_udf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6b554929-ce3f-442a-bcb3-527a0d5c4762","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from typing import Iterator\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\npdf=pd.DataFrame([1,2,3],columns=['x'])\ndf=spark.createDataFrame(pdf)\n\n@pandas_udf('long')\ndef plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n    for x in iterator:\n        yield x+1 \n        \ndf.select(plus_one(\"x\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a631b95-4710-487b-9402-17fef26a1c7a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#Series to Scaler\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n\n\ndf=spark.createDataFrame([(1,2,0),(1,1,1),(2,2,2),(1,2,1),(3,2,1)],(\"id\",\"v\",\"z\"))\n\n@pandas_udf(\"double\")\ndef cal_mean(v:pd.Series) ->float :\n    return v.mean()\n\ndf.select(cal_mean(df['v'])).show()\n\ndf.groupby('id').agg(cal_mean(df['v'])).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"789c4d2d-55f4-40e4-b23d-d83764ed8dcf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["###Pandas Function APIs"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"84644bf5-04cd-434b-b0dd-151f3f3b0318","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df=spark.createDataFrame(\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n    (\"id\", \"v\"))\n\ndef substract_mean(pdf: pd.DataFrame) ->pd.DataFrame:\n    v=pdf.v\n    return pdf.assign( v= v - v.mean())\n\ndf.groupby('id').applyInPandas(substract_mean, schema=df.schema).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ae99880-16f8-4f0a-9d33-936b8a84fed2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#MAP\n\ndf=spark.createDataFrame([(1,21),(1,20),(2,15)],('id','age'))\n\ndef filter_func(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n    for pdf in iterator:\n        yield pdf[pdf.id == 1]\n        \ndf.mapInPandas(filter_func, schema=df.schema).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b12ddc61-7e59-41f5-9d2f-5e0f87494c36","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#Co-grouped map \n\nimport pandas as pd\ndf1=spark.createDataFrame([(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n    (\"time\", \"id\", \"v1\"))\n\ndf2 = spark.createDataFrame(\n    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n    (\"time\", \"id\", \"v2\"))\n\n###Didn't understand \ndef asof_join(left: pd.DataFrame, right: pd.DataFrame) ->pd.DataFrame:\n    return pd.merge_asof(left,right,on='time', by='id')\n\ndf1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(asof_join,schema=\"time int, id int, v1 double, v2 string\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe7a399f-3e94-4f48-9cf7-f96e3c1850c2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["#Pandas API on SPark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6eec25b0-8825-47f1-80eb-98a625f903d1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###Options and Settings"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d27bb118-b4ae-4d13-8e53-4574eac7ae9c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pyspark.pandas as ps\nprint(ps.options.display.max_rows)\n\nps.options.display.max_rows = 100\nprint(ps.options.display.max_rows)\n\nps.set_option('display.max_rows', 200)\n\nprint(ps.get_option(\"display.max_rows\"))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eac44631-9380-451a-9caf-23a5c788467b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":[" #option_context context manager has been exposed through the top-level API, allowing you to execute code with given option values. Option values are restored automatically when you exit the with block:\n\nwith ps.option_context(\"display.max_rows\", 10, \"compute.max_rows\", 5):\n    print(ps.get_option('display.max_rows'))\n    print(ps.get_option('compute.max_rows'))\n    \n\nprint(ps.get_option('compute.max_rows'))\nprint(ps.get_option('display.max_rows'))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"41085b34-807d-4a60-9acf-1d5d5ad0765b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["###Operations on different DataFrames\n\nPandas API on Spark disallows the operations on different DataFrames (or Series) by default to prevent expensive operations. It internally performs a join operation which can be expensive in general.\n\nThis can be enabled by setting compute.ops_on_diff_frames to True to allow such cases. See the examples below."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a9c7c103-f77c-43c5-a251-7b17eab18c02","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pyspark.pandas as ps\nps.set_option('compute.ops_on_diff_frames', True)\n\npsdf1 =ps.range(5)\nprint(psdf1)\npsdf2= ps.DataFrame({'id':[5,4,3]})\n\n(psdf1 - psdf2)\n\nps.reset_option('compute.ops_on_diff_frames')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e82f0a9d-d4b7-4b23-9160-c075e0cc04ec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["##Default Index type\nps.set_option('compute.default_index_type', 'sequence')\npsdf=ps.range(4,10)\nprint(psdf.index)\nps.reset_option('compute.default_index_type')\npsdf=ps.range(4,10)\nprint(psdf.index)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"012c64a5-f295-45c4-b416-9e369404e737","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#distributed-sequence (default): It implements a sequence that increases one by one, by group-by and group-map approach in a distributed manner.\n\nps.set_option('compute.default_index_type','distributed-sequence')\npsdf=ps.range(3)\nps.reset_option('compute.default_index_type')\npsdf.index"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4b34c04-fee8-4d96-ad19-f6caa087ba79","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["#distributed, gwarning : genearlly we should not use this \n\nps.set_option('compute.default_index_type', 'distributed')\npsdf=ps.range(3)\nps.reset_option('compute.default_index_type')\npsdf.index"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7b639cd9-252f-4205-af69-5d8ad1f67724","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["#From/to pandas and PySpark DataFrames"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"97cb8652-a29b-443d-9449-0ae75fbfb126","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#pandas\n\nimport pyspark.pandas as pd\npsdf=pd.range(10)\npsdf\npdf=psdf.to_pandas()\npdf.values\nprint(type(psdf), type(pdf))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d8a794d-76b7-4a3e-b419-a98e6e51d27c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark.pandas as ps\npsdf =ps.range(10)\nprint(type(psdf))\npp=psdf.to_spark()\nprint(type(pp))\n\npdf=psdf.to_pandas()\ntype(pdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2fa34cb4-de41-47b7-8d6e-02040cfc5d3d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["<class 'pyspark.pandas.frame.DataFrame'>\n<class 'pyspark.sql.dataframe.DataFrame'>\nOut[31]: pandas.core.frame.DataFrame"]}],"execution_count":0},{"cell_type":"code","source":["#Pyspark\npsdf = ps.range(10)\ntype(psdf)\nprint(psdf)\n\nsdf=psdf.to_spark().filter('id > 5')\nprint(sdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff1ae4ae-8c0a-432a-80e7-01ff06884c61","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["   id\n0   0\n1   1\n2   2\n3   3\n4   4\n5   5\n6   6\n7   7\n8   8\n9   9\n+---+\n| id|\n+---+\n|  6|\n|  7|\n|  8|\n|  9|\n+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["type(sdf)\npsdf1 = sdf.pandas_api()\nprint(type(psdf1), psdf1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bcff69c8-6cab-400b-9bfb-0f941b6c812e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["<class 'pyspark.pandas.frame.DataFrame'>    id\n0   6\n1   7\n2   8\n3   9\n"]}],"execution_count":0},{"cell_type":"code","source":["psdf = ps.DataFrame({'id':range(10)}, index=range(10))\n\nsdf=psdf.to_spark(index_col='index').filter('id > 5')\nprint(sdf)\n\npsdf1=sdf.pandas_api(index_col='index')\nprint(psdf1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0949dd57-99d0-41fc-9503-441e9eb86842","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+---+\n|index| id|\n+-----+---+\n|    6|  6|\n|    7|  7|\n|    8|  8|\n|    9|  9|\n+-----+---+\n\n       id\nindex    \n6       6\n7       7\n8       8\n9       9\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###Transform and apply a function"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"24de76bc-7a85-4cc8-9136-9577b0da011d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Transform and apply\n\npsdf = ps.DataFrame({'a': [1,2,3], 'b':[4,5,6]})\n\ndef plus_one(pser):\n    return pser + 1  #should always return the same length as input\n\n#print(psdf.transform(plus_one))\n\n\ndef plandas_plus(pser):\n    return pser[pser % 2 == 1]\n\nprint(psdf.apply(plandas_plus,axis='columns'))\n# print(psdf.pandas_on_spark.transform_batch(plus_one))\nprint(psdf.pandas_on_spark.apply_batch(plandas_plus))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ef75601-43bf-4f3c-bd2b-61a498e78829","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["     a    b\n0  1.0  NaN\n1  NaN  5.0\n2  3.0  NaN\n     a    b\n0  1.0  NaN\n1  NaN  5.0\n2  3.0  NaN\n"]}],"execution_count":0},{"cell_type":"markdown","source":["###Type casting between PySpark and pandas API on Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e3611bbd-a7e0-48bf-aef5-63aaf1999faf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["sdf = spark.createDataFrame([(1, 1.0, 1., 1., 1, 1, 1)])\n\nsdf.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e8074c8-4eb3-4023-8963-b06ee515ebf9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[64]: [('_1', 'bigint'),\n ('_2', 'double'),\n ('_3', 'double'),\n ('_4', 'double'),\n ('_5', 'bigint'),\n ('_6', 'bigint'),\n ('_7', 'bigint')]"]}],"execution_count":0},{"cell_type":"code","source":["ps=sdf.pandas_api()\nps.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e349506-99bf-4ebb-9cdc-7b96e18f2366","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[65]: _1      int64\n_2    float64\n_3    float64\n_4    float64\n_5      int64\n_6      int64\n_7      int64\ndtype: object"]}],"execution_count":0},{"cell_type":"code","source":["#Pandas API on Spark currently does not support multiple types of data in single column.\nimport pyspark.pandas as ps\n# ps.Series([1,'A'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"886f43f9-e79e-48ae-a29b-4965b0af84ab","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)\n\u001B[0;32m<command-220042839772850>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSeries\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'A'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/series.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    408\u001B[0m                     \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfastpath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfastpath\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m                 )\n\u001B[0;32m--> 410\u001B[0;31m             \u001B[0minternal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mInternalFrame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    411\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    412\u001B[0m                 \u001B[0minternal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minternal\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumn_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36mfrom_pandas\u001B[0;34m(pdf)\u001B[0m\n\u001B[1;32m   1466\u001B[0m             \u001B[0mdata_columns\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1467\u001B[0m             \u001B[0mdata_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1468\u001B[0;31m         ) = InternalFrame.prepare_pandas_frame(pdf, prefer_timestamp_ntz=prefer_timestamp_ntz)\n\u001B[0m\u001B[1;32m   1469\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1470\u001B[0m         \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfield\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstruct_field\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfield\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mindex_fields\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mdata_fields\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36mprepare_pandas_frame\u001B[0;34m(pdf, retain_index, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m   1567\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1568\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreset_index\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtypes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1569\u001B[0;31m             \u001B[0mspark_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minfer_pd_series_spark_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprefer_timestamp_ntz\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1570\u001B[0m             \u001B[0mreset_index\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDataTypeOps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mspark_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1571\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/typedef/typehints.py\u001B[0m in \u001B[0;36minfer_pd_series_spark_type\u001B[0;34m(pser, dtype, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m    358\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mpser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__UDT__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    359\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 360\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfrom_arrow_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpa\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mArray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpser\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprefer_timestamp_ntz\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    361\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCategoricalDtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    362\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCategoricalDtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.9/site-packages/pyarrow/array.pxi\u001B[0m in \u001B[0;36mpyarrow.lib.Array.from_pandas\u001B[0;34m()\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.9/site-packages/pyarrow/array.pxi\u001B[0m in \u001B[0;36mpyarrow.lib.array\u001B[0;34m()\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.9/site-packages/pyarrow/array.pxi\u001B[0m in \u001B[0;36mpyarrow.lib._ndarray_to_array\u001B[0;34m()\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.9/site-packages/pyarrow/error.pxi\u001B[0m in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n\n\u001B[0;31mArrowInvalid\u001B[0m: Could not convert 'A' with type str: tried to convert to int64","errorSummary":"<span class='ansi-red-fg'>ArrowInvalid</span>: Could not convert 'A' with type str: tried to convert to int64","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)\n\u001B[0;32m<command-220042839772850>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSeries\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'A'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/series.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    408\u001B[0m                     \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfastpath\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfastpath\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m                 )\n\u001B[0;32m--> 410\u001B[0;31m             \u001B[0minternal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mInternalFrame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    411\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    412\u001B[0m                 \u001B[0minternal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minternal\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumn_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36mfrom_pandas\u001B[0;34m(pdf)\u001B[0m\n\u001B[1;32m   1466\u001B[0m             \u001B[0mdata_columns\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1467\u001B[0m             \u001B[0mdata_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1468\u001B[0;31m         ) = InternalFrame.prepare_pandas_frame(pdf, prefer_timestamp_ntz=prefer_timestamp_ntz)\n\u001B[0m\u001B[1;32m   1469\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1470\u001B[0m         \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStructType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfield\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstruct_field\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfield\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mindex_fields\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mdata_fields\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36mprepare_pandas_frame\u001B[0;34m(pdf, retain_index, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m   1567\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1568\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreset_index\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtypes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1569\u001B[0;31m             \u001B[0mspark_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minfer_pd_series_spark_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprefer_timestamp_ntz\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1570\u001B[0m             \u001B[0mreset_index\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDataTypeOps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mspark_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1571\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/typedef/typehints.py\u001B[0m in \u001B[0;36minfer_pd_series_spark_type\u001B[0;34m(pser, dtype, prefer_timestamp_ntz)\u001B[0m\n\u001B[1;32m    358\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mpser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__UDT__\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    359\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 360\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfrom_arrow_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpa\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mArray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpser\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mprefer_timestamp_ntz\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    361\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCategoricalDtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    362\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCategoricalDtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.9/site-packages/pyarrow/array.pxi\u001B[0m in \u001B[0;36mpyarrow.lib.Array.from_pandas\u001B[0;34m()\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.9/site-packages/pyarrow/array.pxi\u001B[0m in \u001B[0;36mpyarrow.lib.array\u001B[0;34m()\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.9/site-packages/pyarrow/array.pxi\u001B[0m in \u001B[0;36mpyarrow.lib._ndarray_to_array\u001B[0;34m()\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.9/site-packages/pyarrow/error.pxi\u001B[0m in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n\n\u001B[0;31mArrowInvalid\u001B[0m: Could not convert 'A' with type str: tried to convert to int64"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nimport pyspark.pandas as ps\nspark=SparkSession.builder.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6d1bc05-2c00-463e-8e65-af05c3f37737","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def pandas_div(pdf) -> ps.DataFrame[float, float]:\n    return pdf[['B', 'C']] / pdf[['B', 'C']]\n\ndf=ps.DataFrame({'A': ['a', 'a', 'b'], 'B': [1, 2, 3], 'C': [4, 6, 5]})\n#print(df)\ntype(df.groupby('A').apply(pandas_div))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bf68e258-edfb-45dd-9718-813a022f3747","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["   A  B  C\n0  a  1  4\n1  a  2  6\n2  b  3  5\nOut[12]: pyspark.pandas.frame.DataFrame"]}],"execution_count":0},{"cell_type":"code","source":["df = ps.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\ndf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0a2284e7-6b0f-4c8d-b188-3e7495b19d0c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def transform(pdf) -> pd.DataFrame['id':int, 'A':int]:\n    pdf['A'] = pdf.id + 1\n    return pdf\n\nprint(ps.range(5))\nps.range(5).pandas_on_spark.apply_batch(transform)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3c337a3a-c3af-4d3b-93d0-a94f1d9093b9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["   id\n0   0\n1   1\n2   2\n3   3\n4   4\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>A</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>A</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\n\npdf=pd.DataFrame({'id':range(5)})\nprint(pdf)\n\nsample=pdf.copy()\nsample['A']=sample.id + 1\nprint(sample)\nprint(pdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"232f7abc-d4e9-4dc5-a141-c3e156651681","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["   id\n0   0\n1   1\n2   2\n3   3\n4   4\n   id  A\n0   0  1\n1   1  2\n2   2  3\n3   3  4\n4   4  5\n   id\n0   0\n1   1\n2   2\n3   3\n4   4\n"]}],"execution_count":0},{"cell_type":"code","source":["def transform(pdf)->pd.DataFrame[int,[int,int]]:\n    pdf['a']=pdf.id+1\n    return pdf\n\nps.from_pandas(pdf).pandas_on_spark.apply_batch(transform)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0753fb79-5024-4a10-b140-d8df1f034a09","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>c0</th>\n      <th>c1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>c0</th>\n      <th>c1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def transform(pdf) ->pd.DataFrame[sample.index.dtype, sample.dtypes]:\n    pdf['a']=pdf.id+1\n    return pdf\n\nps.from_pandas(pdf).pandas_on_spark.apply_batch(transform)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51165752-6d70-48ee-a369-d4c6df11fde6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>c0</th>\n      <th>c1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>c0</th>\n      <th>c1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["midx = pd.MultiIndex.from_arrays([(1, 1, 2), (1.5, 4.5, 7.5)],names=(\"int\",\"float\"))\n\ndef transform(pdf) -> pd.DataFrame[[int, float], [int, int]]:\n    pdf[\"a\"] = pdf.id + 1\n    return pdf\n\nps.from_pandas(pdf).pandas_on_spark.apply_batch(transform)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3ede09e7-5ea5-4c62-85ca-3b76df242d3d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["/databricks/spark/python/pyspark/sql/pandas/conversion.py:208: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\n  An error occurred while calling o3187.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:454)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.GeneratedMethodAccessor745.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 52.0 failed 1 times, most recent failure: Lost task 1.0 in stage 52.0 (TID 156) (ip-10-172-165-213.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'ValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/generic.py\", line 5502, in __setattr__\n    pass\n  File \"pandas/_libs/properties.pyx\", line 70, in pandas._libs.properties.AxisProperty.__set__\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/generic.py\", line 766, in _set_axis\n    self._mgr.set_axis(axis, labels)\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 216, in set_axis\n    self._validate_set_axis(axis, new_labels)\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/internals/base.py\", line 57, in _validate_set_axis\n    raise ValueError(\nValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:110)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:169)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3312)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3244)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3235)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3235)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1424)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1424)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1424)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3524)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3462)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3450)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1169)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1157)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2727)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2710)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2822)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:4225)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:4229)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:4195)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4292)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:782)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4290)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:243)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:392)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:188)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:342)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4290)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:4195)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:4194)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: 'ValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/generic.py\", line 5502, in __setattr__\n    pass\n  File \"pandas/_libs/properties.pyx\", line 70, in pandas._libs.properties.AxisProperty.__set__\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/generic.py\", line 766, in _set_axis\n    self._mgr.set_axis(axis, labels)\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 216, in set_axis\n    self._validate_set_axis(axis, new_labels)\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/internals/base.py\", line 57, in _validate_set_axis\n    raise ValueError(\nValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:110)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:169)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\n  warn(msg)\n/databricks/spark/python/pyspark/sql/pandas/conversion.py:208: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\n  An error occurred while calling o3237.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:454)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.GeneratedMethodAccessor745.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 54.0 failed 1 times, most recent failure: Lost task 2.0 in stage 54.0 (TID 162) (ip-10-172-165-213.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'ValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/generic.py\", line 5502, in __setattr__\n    pass\n  File \"pandas/_libs/properties.pyx\", line 70, in pandas._libs.properties.AxisProperty.__set__\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/generic.py\", line 766, in _set_axis\n    self._mgr.set_axis(axis, labels)\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 216, in set_axis\n    self._validate_set_axis(axis, new_labels)\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/internals/base.py\", line 57, in _validate_set_axis\n    raise ValueError(\nValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:110)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:169)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3312)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3244)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3235)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3235)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1424)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1424)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1424)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3524)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3462)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3450)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1169)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1157)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2727)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2710)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2822)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:4225)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:4229)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:4195)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4292)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:782)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4290)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:243)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:392)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:188)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:342)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4290)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:4195)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:4194)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.api.python.PythonException: 'ValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements'. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/generic.py\", line 5502, in __setattr__\n    pass\n  File \"pandas/_libs/properties.pyx\", line 70, in pandas._libs.properties.AxisProperty.__set__\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/generic.py\", line 766, in _set_axis\n    self._mgr.set_axis(axis, labels)\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/internals/managers.py\", line 216, in set_axis\n    self._validate_set_axis(axis, new_labels)\n  File \"/databricks/python/lib/python3.9/site-packages/pandas/core/internals/base.py\", line 57, in _validate_set_axis\n    raise ValueError(\nValueError: Length mismatch: Expected axis has 3 elements, new values have 4 elements\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:110)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:81)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:169)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$4(Task.scala:137)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:125)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:137)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\n  warn(msg)\n"]}],"execution_count":0},{"cell_type":"code","source":["import sqlite3\n\ncon = sqlite3.connect('example.db')\ncur = con.cursor()\n# Create table\ncur.execute(\n    '''CREATE TABLE stocks\n       (date text, trans text, symbol text, qty real, price real)''')\n# Insert a row of data\ncur.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n# Save (commit) the changes\ncon.commit()\ncon.close()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f88e4179-d834-4268-815c-02ace724efb8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["!curl -O https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.34.0/sqlite-jdbc-3.34.0.jar"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81c4d75f-36e5-4f93-8b2b-efaaf8f0aa5f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 7125k  100 7125k    0     0  46.0M      0 --:--:-- --:--:-- --:--:-- 46.0M\r\n"]}],"execution_count":0},{"cell_type":"code","source":["import os\n\nfrom pyspark.sql import SparkSession\n\n(SparkSession.builder\n    .master(\"local\")\n    .appName(\"SQLite JDBC\")\n    .config(\n        \"spark.jars\",\n        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd()))\n    .config(\n        \"spark.driver.extraClassPath\",\n        \"{}/sqlite-jdbc-3.34.0.jar\".format(os.getcwd()))\n    .getOrCreate())\n#Now, you’re ready to read the table:\n\nimport pyspark.pandas as ps\n\ndf = ps.read_sql(\"stocks\", con=\"jdbc:sqlite:{}/example.db\".format(os.getcwd()))\ndf"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a45cf228-b409-4648-8f18-1563b57f3a24","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>trans</th>\n      <th>symbol</th>\n      <th>qty</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2006-01-05</td>\n      <td>BUY</td>\n      <td>RHAT</td>\n      <td>100.0</td>\n      <td>35.14</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>trans</th>\n      <th>symbol</th>\n      <th>qty</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2006-01-05</td>\n      <td>BUY</td>\n      <td>RHAT</td>\n      <td>100.0</td>\n      <td>35.14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.price += 1\ndf.spark.to_spark_io(\n    format=\"jdbc\", mode=\"append\",\n    dbtable=\"stocks\", url=\"jdbc:sqlite:{}/example.db\".format(os.getcwd()))\nps.read_sql(\"stocks\", con=\"jdbc:sqlite:{}/example.db\".format(os.getcwd()))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bf91d709-1c59-4b56-8dca-088ead99b19e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>trans</th>\n      <th>symbol</th>\n      <th>qty</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2006-01-05</td>\n      <td>BUY</td>\n      <td>RHAT</td>\n      <td>100.0</td>\n      <td>35.14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2006-01-05</td>\n      <td>BUY</td>\n      <td>RHAT</td>\n      <td>100.0</td>\n      <td>36.14</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>trans</th>\n      <th>symbol</th>\n      <th>qty</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2006-01-05</td>\n      <td>BUY</td>\n      <td>RHAT</td>\n      <td>100.0</td>\n      <td>35.14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2006-01-05</td>\n      <td>BUY</td>\n      <td>RHAT</td>\n      <td>100.0</td>\n      <td>36.14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#Best Practice"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af5059a5-240b-4e6a-bce9-0a3b8f0175cf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark import SparkConf, SparkContext\nconf = SparkConf()\nconf.set('spark.executor.memory', '2g')\n# Pandas API on Spark automatically uses this Spark context with the configurations set.\nSparkContext(conf=conf)\n\nimport pyspark.pandas as ps"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cf59a95a-0e00-4200-adf5-e137b16c57c2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-2673134026439754>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'spark.executor.memory'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'2g'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Pandas API on Spark automatically uses this Spark context with the configurations set.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001B[0m\n\u001B[1;32m    197\u001B[0m             )\n\u001B[1;32m    198\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 199\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    200\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m             self._do_init(\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    476\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    477\u001B[0m                     \u001B[0;31m# Raise error if there is already a running Spark context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 478\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m    479\u001B[0m                         \u001B[0;34m\"Cannot run multiple SparkContexts at once; \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    480\u001B[0m                         \u001B[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-2673134026439754>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'spark.executor.memory'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'2g'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m# Pandas API on Spark automatically uses this Spark context with the configurations set.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001B[0m\n\u001B[1;32m    197\u001B[0m             )\n\u001B[1;32m    198\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 199\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    200\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m             self._do_init(\n\n\u001B[0;32m/databricks/spark/python/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    476\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    477\u001B[0m                     \u001B[0;31m# Raise error if there is already a running Spark context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 478\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m    479\u001B[0m                         \u001B[0;34m\"Cannot run multiple SparkContexts at once; \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    480\u001B[0m                         \u001B[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 "]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark.pandas as ps\npsdf=ps.DataFrame({'id':range(10)})\npsdf['id']=psdf[psdf.id > 5]\npsdf['id']=psdf['id']+(10*psdf['id']+psdf['id'])\npsdf=psdf.groupby('id').head(2)\npsdf.spark.explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4a50c0c3-615e-4ba0-a698-3b922d4284a4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-1194836937574392>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mpsdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mid\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mpsdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/frame.py\u001B[0m in \u001B[0;36m__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m  12416\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_psser_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthis_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthis_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m  12417\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m> 12418\u001B[0;31m             \u001B[0mpsdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0malign_diff_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0massign_columns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfillna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"left\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m  12419\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m  12420\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/utils.py\u001B[0m in \u001B[0;36malign_diff_frames\u001B[0;34m(resolve_func, this, that, fillna, how, preserve_order_column)\u001B[0m\n\u001B[1;32m    385\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    386\u001B[0m     \u001B[0;31m# 1. Perform the join given two dataframes.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 387\u001B[0;31m     \u001B[0mcombined\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcombine_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreserve_order_column\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpreserve_order_column\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    388\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    389\u001B[0m     \u001B[0;31m# 2. Apply the given function to transform the columns in a batch and keep the new columns.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/utils.py\u001B[0m in \u001B[0;36mcombine_frames\u001B[0;34m(this, how, preserve_order_column, *args)\u001B[0m\n\u001B[1;32m    309\u001B[0m         )\n\u001B[1;32m    310\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 311\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mERROR_MESSAGE_CANNOT_COMBINE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: Cannot combine the series or dataframe because it comes from a different dataframe. In order to allow this operation, enable 'compute.ops_on_diff_frames' option.","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: Cannot combine the series or dataframe because it comes from a different dataframe. In order to allow this operation, enable 'compute.ops_on_diff_frames' option.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n\u001B[0;32m<command-1194836937574392>\u001B[0m in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mpsdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mid\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mpsdf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'id'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/frame.py\u001B[0m in \u001B[0;36m__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m  12416\u001B[0m                         \u001B[0;32myield\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mpsdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_psser_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthis_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthis_label\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m  12417\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m> 12418\u001B[0;31m             \u001B[0mpsdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0malign_diff_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0massign_columns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfillna\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"left\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m  12419\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m  12420\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/utils.py\u001B[0m in \u001B[0;36malign_diff_frames\u001B[0;34m(resolve_func, this, that, fillna, how, preserve_order_column)\u001B[0m\n\u001B[1;32m    385\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    386\u001B[0m     \u001B[0;31m# 1. Perform the join given two dataframes.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 387\u001B[0;31m     \u001B[0mcombined\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcombine_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mthis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhow\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mhow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpreserve_order_column\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpreserve_order_column\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    388\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    389\u001B[0m     \u001B[0;31m# 2. Apply the given function to transform the columns in a batch and keep the new columns.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/utils.py\u001B[0m in \u001B[0;36mcombine_frames\u001B[0;34m(this, how, preserve_order_column, *args)\u001B[0m\n\u001B[1;32m    309\u001B[0m         )\n\u001B[1;32m    310\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 311\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mERROR_MESSAGE_CANNOT_COMBINE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mValueError\u001B[0m: Cannot combine the series or dataframe because it comes from a different dataframe. In order to allow this operation, enable 'compute.ops_on_diff_frames' option."]}}],"execution_count":0},{"cell_type":"code","source":["# psdf = ps.DataFrame({'a': [1, 2], 'A':[3, 4]}) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89575ac4-2ac3-4e01-aade-6e07919c7b77","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-4301303750679896>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpsdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'A'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/frame.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    449\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    450\u001B[0m                 \u001B[0mpdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 451\u001B[0;31m             \u001B[0minternal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mInternalFrame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    453\u001B[0m         \u001B[0mobject\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__setattr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"_internal_frame\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minternal\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36mfrom_pandas\u001B[0;34m(pdf)\u001B[0m\n\u001B[1;32m   1477\u001B[0m             \u001B[0mindex_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1478\u001B[0m             \u001B[0mcolumn_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_labels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1479\u001B[0;31m             \u001B[0mdata_spark_columns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_columns\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1480\u001B[0m             \u001B[0mdata_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1481\u001B[0m             \u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1477\u001B[0m             \u001B[0mindex_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1478\u001B[0m             \u001B[0mcolumn_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_labels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1479\u001B[0;31m             \u001B[0mdata_spark_columns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_columns\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1480\u001B[0m             \u001B[0mdata_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1481\u001B[0m             \u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/utils.py\u001B[0m in \u001B[0;36mscol_for\u001B[0;34m(sdf, column_name)\u001B[0m\n\u001B[1;32m    599\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mSparkDataFrame\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumn_name\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m     \u001B[0;34m\"\"\"Return Spark Column for the given column name.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 601\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0msdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"`{}`\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumn_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    602\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    603\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_local\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logging\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# no need to log since this should be internal call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2049\u001B[0m         \"\"\"\n\u001B[1;32m   2050\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2051\u001B[0;31m             \u001B[0mjc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2052\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2053\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Reference 'a' is ambiguous, could be: a, a.","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Reference 'a' is ambiguous, could be: a, a.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-4301303750679896>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mpsdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'A'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/frame.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    449\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    450\u001B[0m                 \u001B[0mpdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 451\u001B[0;31m             \u001B[0minternal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mInternalFrame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    453\u001B[0m         \u001B[0mobject\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__setattr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"_internal_frame\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minternal\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36mfrom_pandas\u001B[0;34m(pdf)\u001B[0m\n\u001B[1;32m   1477\u001B[0m             \u001B[0mindex_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1478\u001B[0m             \u001B[0mcolumn_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_labels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1479\u001B[0;31m             \u001B[0mdata_spark_columns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_columns\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1480\u001B[0m             \u001B[0mdata_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1481\u001B[0m             \u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1477\u001B[0m             \u001B[0mindex_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1478\u001B[0m             \u001B[0mcolumn_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_labels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1479\u001B[0;31m             \u001B[0mdata_spark_columns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_columns\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1480\u001B[0m             \u001B[0mdata_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1481\u001B[0m             \u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/utils.py\u001B[0m in \u001B[0;36mscol_for\u001B[0;34m(sdf, column_name)\u001B[0m\n\u001B[1;32m    599\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mSparkDataFrame\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumn_name\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m     \u001B[0;34m\"\"\"Return Spark Column for the given column name.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 601\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0msdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"`{}`\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumn_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    602\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    603\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_local\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logging\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# no need to log since this should be internal call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2049\u001B[0m         \"\"\"\n\u001B[1;32m   2050\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2051\u001B[0;31m             \u001B[0mjc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2052\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2053\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Reference 'a' is ambiguous, could be: a, a."]}}],"execution_count":0},{"cell_type":"code","source":["builder = builder.config(\"spark.sql.caseSensitive\", \"true\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81bcf802-898e-4178-a7c2-be6b9d683cc6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4301303750679897>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mbuilder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"spark.sql.caseSensitive\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"true\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'builder' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'builder' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4301303750679897>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mbuilder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"spark.sql.caseSensitive\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"true\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'builder' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["builder = SparkSession.builder.appName(\"pandas-on-spark\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d7bdce7f-54c7-45b2-9d32-e561c1893602","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4301303750679898>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mbuilder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappName\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pandas-on-spark\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'SparkSession' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'SparkSession' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4301303750679898>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mbuilder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappName\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pandas-on-spark\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'SparkSession' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["builder = builder.config(\"spark.sql.caseSensitive\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c6822cc2-3bbe-40b2-871c-9f1463e84229","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4301303750679899>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mbuilder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"spark.sql.caseSensitive\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"true\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'builder' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'builder' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n\u001B[0;32m<command-4301303750679899>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mbuilder\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"spark.sql.caseSensitive\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"true\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mNameError\u001B[0m: name 'builder' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["\nsdf = ps.DataFrame({'a': [1, 2], 'A':[3, 4]}) "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"425b638f-ce30-46c5-82a2-81431d339c54","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-4301303750679900>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'A'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/frame.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    449\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    450\u001B[0m                 \u001B[0mpdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 451\u001B[0;31m             \u001B[0minternal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mInternalFrame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    453\u001B[0m         \u001B[0mobject\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__setattr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"_internal_frame\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minternal\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36mfrom_pandas\u001B[0;34m(pdf)\u001B[0m\n\u001B[1;32m   1477\u001B[0m             \u001B[0mindex_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1478\u001B[0m             \u001B[0mcolumn_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_labels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1479\u001B[0;31m             \u001B[0mdata_spark_columns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_columns\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1480\u001B[0m             \u001B[0mdata_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1481\u001B[0m             \u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1477\u001B[0m             \u001B[0mindex_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1478\u001B[0m             \u001B[0mcolumn_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_labels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1479\u001B[0;31m             \u001B[0mdata_spark_columns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_columns\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1480\u001B[0m             \u001B[0mdata_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1481\u001B[0m             \u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/utils.py\u001B[0m in \u001B[0;36mscol_for\u001B[0;34m(sdf, column_name)\u001B[0m\n\u001B[1;32m    599\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mSparkDataFrame\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumn_name\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m     \u001B[0;34m\"\"\"Return Spark Column for the given column name.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 601\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0msdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"`{}`\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumn_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    602\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    603\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_local\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logging\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# no need to log since this should be internal call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2049\u001B[0m         \"\"\"\n\u001B[1;32m   2050\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2051\u001B[0;31m             \u001B[0mjc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2052\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2053\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Reference 'a' is ambiguous, could be: a, a.","errorSummary":"<span class='ansi-red-fg'>AnalysisException</span>: Reference 'a' is ambiguous, could be: a, a.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-4301303750679900>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'a'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'A'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/frame.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    449\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    450\u001B[0m                 \u001B[0mpdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 451\u001B[0;31m             \u001B[0minternal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mInternalFrame\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_pandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    453\u001B[0m         \u001B[0mobject\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__setattr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"_internal_frame\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minternal\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36mfrom_pandas\u001B[0;34m(pdf)\u001B[0m\n\u001B[1;32m   1477\u001B[0m             \u001B[0mindex_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1478\u001B[0m             \u001B[0mcolumn_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_labels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1479\u001B[0;31m             \u001B[0mdata_spark_columns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_columns\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1480\u001B[0m             \u001B[0mdata_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1481\u001B[0m             \u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/internal.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1477\u001B[0m             \u001B[0mindex_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mindex_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1478\u001B[0m             \u001B[0mcolumn_labels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_labels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1479\u001B[0;31m             \u001B[0mdata_spark_columns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcol\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdata_columns\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1480\u001B[0m             \u001B[0mdata_fields\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdata_fields\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1481\u001B[0m             \u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolumn_label_names\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/pandas/utils.py\u001B[0m in \u001B[0;36mscol_for\u001B[0;34m(sdf, column_name)\u001B[0m\n\u001B[1;32m    599\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mscol_for\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msdf\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mSparkDataFrame\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolumn_name\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m     \u001B[0;34m\"\"\"Return Spark Column for the given column name.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 601\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0msdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"`{}`\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumn_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    602\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    603\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_local\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logging\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# no need to log since this should be internal call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2049\u001B[0m         \"\"\"\n\u001B[1;32m   2050\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2051\u001B[0;31m             \u001B[0mjc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2052\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2053\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mColumn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Reference 'a' is ambiguous, could be: a, a."]}}],"execution_count":0},{"cell_type":"code","source":["#From rdd to Dataframe\n\ndept = [('Fiannce',10),('Marketing',10),('sales',30),('IT',40)]\nrdd=spark.sparkContext.parallelize(dept)\n\ndf=rdd.toDF()\ndf.printSchema()\ndf.show(truncate=False)\n\n\ndeptColumns=['dept_name','dept_id']\ndf2 = rdd.toDF(deptColumns)\ndf2.show()\n\nfrom pyspark.sql.types import StructType, StructField, StringType\ndeptSchema = StructType([\n    StructField('dept_name', StringType(), True),\n    StructField('dept_id', StringType(), True)\n])\ndeptDF = spark.createDataFrame(rdd, schema=deptSchema)\n\ndeptDF.printSchema()\ndeptDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23d1888c-5c7a-47bf-a069-db80430d9b7d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n\n+---------+---+\n|_1       |_2 |\n+---------+---+\n|Fiannce  |10 |\n|Marketing|10 |\n|sales    |30 |\n|IT       |40 |\n+---------+---+\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Fiannce|     10|\n|Marketing|     10|\n|    sales|     30|\n|       IT|     40|\n+---------+-------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: string (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Fiannce  |10     |\n|Marketing|10     |\n|sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Convert PySpark DataFrame to Pandas\n\n#Convert Spark Nested Struct DataFrame to Pandas\n\nfrom pyspark.sql.types import StructType, StringType, StructField, IntegerType\n\ndataset = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n]\n\nschemaStruct = StructType([\n    StructField('name', StructType([\n        StructField('firstname', StringType(), True),\n        StructField('middlename', StringType(), True),\n        StructField('lastname', StringType(), True)\n    ])),\n    StructField('dob', StringType(), True),\n    StructField('gender', StringType(), True),\n    StructField('Salary', StringType(), True)\n])\n\ndf=spark.createDataFrame(data=dataset, schema=schemaStruct)\ndf.printSchema()\npdf2=df.toPandas()\nprint(pdf2)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"79149915-a898-4f76-b71e-700dd20a8b67","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- Salary: string (nullable = true)\n\n                                                name    dob gender Salary\n0  {'firstname': 'James', 'middlename': '', 'last...  36636      M   3000\n1  {'firstname': 'Michael', 'middlename': 'Rose',...  40288      M   4000\n2  {'firstname': 'Robert', 'middlename': '', 'las...  42114      M   4000\n3  {'firstname': 'Maria', 'middlename': 'Anne', '...  39192      F   4000\n4  {'firstname': 'Jen', 'middlename': 'Mary', 'la...             F     -1\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark show() – Display DataFrame Contents in Table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f96cae4e-4e99-4023-9d09-c9fa2dd5f829","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1cc90a9c-ec75-45cb-9147-442ff16b0b2e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+-----+------+------+\n|                name|  dob|gender|Salary|\n+--------------------+-----+------+------+\n|    {James, , Smith}|36636|     M|  3000|\n|   {Michael, Rose, }|40288|     M|  4000|\n|{Robert, , Williams}|42114|     M|  4000|\n|{Maria, Anne, Jones}|39192|     F|  4000|\n|  {Jen, Mary, Brown}|     |     F|    -1|\n+--------------------+-----+------+------+\n\n+--------------------+-----+------+------+\n|name                |dob  |gender|Salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3000  |\n|{Michael, Rose, }   |40288|M     |4000  |\n|{Robert, , Williams}|42114|M     |4000  |\n|{Maria, Anne, Jones}|39192|F     |4000  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Default - displays 20 rows and \n# 20 charactes from column value \ndf.show()\n\n#Display full column contents\ndf.show(truncate=False)\n\n# Display 2 rows and full column contents\ndf.show(2,truncate=False) \n\n# Display 2 rows & column values 25 characters\ndf.show(2,truncate=25) \n\n# Display DataFrame rows & columns vertically\ndf.show(n=3,truncate=25,vertical=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4bd03c2d-4c22-4659-8d1a-4eeda3da6e11","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+-----+------+------+\n|                name|  dob|gender|Salary|\n+--------------------+-----+------+------+\n|    {James, , Smith}|36636|     M|  3000|\n|   {Michael, Rose, }|40288|     M|  4000|\n|{Robert, , Williams}|42114|     M|  4000|\n|{Maria, Anne, Jones}|39192|     F|  4000|\n|  {Jen, Mary, Brown}|     |     F|    -1|\n+--------------------+-----+------+------+\n\n+--------------------+-----+------+------+\n|name                |dob  |gender|Salary|\n+--------------------+-----+------+------+\n|{James, , Smith}    |36636|M     |3000  |\n|{Michael, Rose, }   |40288|M     |4000  |\n|{Robert, , Williams}|42114|M     |4000  |\n|{Maria, Anne, Jones}|39192|F     |4000  |\n|{Jen, Mary, Brown}  |     |F     |-1    |\n+--------------------+-----+------+------+\n\n+-----------------+-----+------+------+\n|name             |dob  |gender|Salary|\n+-----------------+-----+------+------+\n|{James, , Smith} |36636|M     |3000  |\n|{Michael, Rose, }|40288|M     |4000  |\n+-----------------+-----+------+------+\nonly showing top 2 rows\n\n+-----------------+-----+------+------+\n|             name|  dob|gender|Salary|\n+-----------------+-----+------+------+\n| {James, , Smith}|36636|     M|  3000|\n|{Michael, Rose, }|40288|     M|  4000|\n+-----------------+-----+------+------+\nonly showing top 2 rows\n\n-RECORD 0----------------------\n name   | {James, , Smith}     \n dob    | 36636                \n gender | M                    \n Salary | 3000                 \n-RECORD 1----------------------\n name   | {Michael, Rose, }    \n dob    | 40288                \n gender | M                    \n Salary | 4000                 \n-RECORD 2----------------------\n name   | {Robert, , Williams} \n dob    | 42114                \n gender | M                    \n Salary | 4000                 \nonly showing top 3 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\ncolumns=['seqNo', 'quote']\ndata=[('1','ANCD'),('2','BBBB'),('3','CCCC'),('4','DDDD')]\ndf=spark.createDataFrame(data,columns)\ndf.show()\ndf.show(2,truncate=False)\ndf.show(2,truncate=False,vertical=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0fae34b-6a01-4f43-b0ad-ccea76f6ea16","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+\n|seqNo|quote|\n+-----+-----+\n|    1| ANCD|\n|    2| BBBB|\n|    3| CCCC|\n|    4| DDDD|\n+-----+-----+\n\n+-----+-----+\n|seqNo|quote|\n+-----+-----+\n|1    |ANCD |\n|2    |BBBB |\n+-----+-----+\nonly showing top 2 rows\n\n-RECORD 0-----\n seqNo | 1    \n quote | ANCD \n-RECORD 1-----\n seqNo | 2    \n quote | BBBB \nonly showing top 2 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import ArrayType,MapType\n#Using SQL ArrayType and MapType\n\narrayStructSchema=StructType([\n    StructField('name', StructType([\n        StructField('firstName' ,StringType(),True),\n        StructField('middlename', StringType(), True),\n        StructField('lastname', StringType(), True)      \n    ])),\n    StructField('hobbies', ArrayType(StringType()), True),\n    StructField('properties', MapType(StringType(),StringType()), True)\n])\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff900527-7d18-42d4-ad02-3b8e2d934afe","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-3144980275726779>\u001B[0m in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m ])\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0marrayStructSchema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m: 'StructType' object has no attribute 'printSchema'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'StructType' object has no attribute 'printSchema'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-3144980275726779>\u001B[0m in \u001B[0;36m<cell line: 14>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m ])\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0marrayStructSchema\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprintSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m: 'StructType' object has no attribute 'printSchema'"]}}],"execution_count":0},{"cell_type":"code","source":["#PySpark Column Class | Operators & Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d22ca973-3329-4c30-ae84-024f141312e0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import lit\ncalObj = lit('sparkbyexample.com')\n\n\nfrom pyspark.sql import Row\ndata=[Row(name='James',prop=Row(hair='black',eye='blue')),\n     Row(name='Ann',prop=Row(hair='grey',eye='black'))]\n\ndf=spark.createDataFrame(data)\ndf.printSchema()\n\n\n##Access Struct column \n\ndf.select(df.prop.hair).show()\ndf.select(df[\"prop.hair\"]).show()\n\nfrom pyspark.sql.functions import col\n\ndf.select(col('prop.hair')).show()\ndf.select(col('prop.*')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef68172c-d692-433b-8e04-88770de0fca9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- prop: struct (nullable = true)\n |    |-- hair: string (nullable = true)\n |    |-- eye: string (nullable = true)\n\n+---------+\n|prop.hair|\n+---------+\n|    black|\n|     grey|\n+---------+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n+-----+\n| hair|\n+-----+\n|black|\n| grey|\n+-----+\n\n+-----+-----+\n| hair|  eye|\n+-----+-----+\n|black| blue|\n| grey|black|\n+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["data=[(100,2,1),(200,3,4),(300,4,4)]\ndf=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\ndf.select(df.col1 + df.col2).show()\ndf.select(df.col1 - df.col2).show()\ndf.select(df.col1 / df.col2).show()\n\ndf.select(df.col2==df.col3).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0db30944-7b95-4c1d-bdab-a2c8b94f49e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n|(col1 + col2)|\n+-------------+\n|          102|\n|          203|\n|          304|\n+-------------+\n\n+-------------+\n|(col1 - col2)|\n+-------------+\n|           98|\n|          197|\n|          296|\n+-------------+\n\n+-----------------+\n|    (col1 / col2)|\n+-----------------+\n|             50.0|\n|66.66666666666667|\n|             75.0|\n+-----------------+\n\n+-------------+\n|(col2 = col3)|\n+-------------+\n|        false|\n|        false|\n|         true|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark Select Columns From DataFrame\n\n\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n  ]\ncolumns = [\"firstname\",\"lastname\",\"country\",\"state\"]\ndf = spark.createDataFrame(data = data, schema = columns)\n#df.show(truncate=False)\n\ndf.select('firstname','lastname').show()\ndf.select(df.firstname,df.lastname).show()\ndf.select(df['firstname'],df['lastname']).show()\n\nfrom pyspark.sql import SparkSession\n\n\ndf.select(col('firstname'),col('lastname')).show()\n\ndf.select(df.col)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df58d21d-264b-4589-90bc-ea401f2c4724","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|    Rose|\n|   Robert|Williams|\n|    Maria|   Jones|\n+---------+--------+\n\n+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|    Rose|\n|   Robert|Williams|\n|    Maria|   Jones|\n+---------+--------+\n\n+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|    Rose|\n|   Robert|Williams|\n|    Maria|   Jones|\n+---------+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf.select(col('firstname'),col('lastname')).show()\n\ndf.select(df.colRegex(\"`^.*name*`\")).show()\n\ndf.select(*columns).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de41da98-527e-4b96-b084-eef5837933d2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|    Rose|\n|   Robert|Williams|\n|    Maria|   Jones|\n+---------+--------+\n\n+---------+--------+\n|firstname|lastname|\n+---------+--------+\n|    James|   Smith|\n|  Michael|    Rose|\n|   Robert|Williams|\n|    Maria|   Jones|\n+---------+--------+\n\n+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Select Nested Struct Columns from PySpark\n\n\ndata = [\n        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n        ]\n\nfrom pyspark.sql.types import StructType,StructField, StringType        \nschema = StructType([\n    StructField('name', StructType([\n         StructField('firstname', StringType(), True),\n         StructField('middlename', StringType(), True),\n         StructField('lastname', StringType(), True)\n         ])),\n     StructField('state', StringType(), True),\n     StructField('gender', StringType(), True)\n     ])\ndf2 = spark.createDataFrame(data = data, schema = schema)\ndf2.printSchema()\n#df2.show(truncate=False) # shows all columns\n\n\n#df2.select('name').show()\n#df2.select('name.firstname', 'name.lastname').show() #sub columns \n\ndf2.select(\"name.*\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"97c13d9c-f504-46df-bf92-939a6d31104a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- state: string (nullable = true)\n |-- gender: string (nullable = true)\n\n+---------+----------+--------+\n|firstname|middlename|lastname|\n+---------+----------+--------+\n|    James|      null|   Smith|\n|     Anna|      Rose|        |\n|    Julia|          |Williams|\n|    Maria|      Anne|   Jones|\n|      Jen|      Mary|   Brown|\n|     Mike|      Mary|Williams|\n+---------+----------+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark Collect() – Retrieve data from DataFrame\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\n\ndeptColumns = ['dept_name','dept_id']\ndeptDF = spark.createDataFrame(data=dept, schema=deptColumns)\ndeptDF.show(truncate=False)\n\ndataCollect=deptDF.collect() ##REturns array type\nprint(dataCollect)\n\nfor row in dataCollect:\n    print(row['dept_name']+ \", \"+str(row['dept_id']))\n    \ndeptDF.collect()[0][0]\n\ndataCollect = deptDF.select('dept_name').collect()\nprint(dataCollect)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04bcb805-99fb-4052-b88e-86f6ffe31c10","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\nFinance, 10\nMarketing, 20\nSales, 30\nIT, 40\n[Row(dept_name='Finance'), Row(dept_name='Marketing'), Row(dept_name='Sales'), Row(dept_name='IT')]\n"]}],"execution_count":0},{"cell_type":"code","source":["deptDF.withColumn('TEST',lit(2)).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90d8d9ca-82b2-4ef1-9fa2-e8fcd2641a5a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+-------+----+\n|dept_name|dept_id|TEST|\n+---------+-------+----+\n|  Finance|     10|   2|\n|Marketing|     20|   2|\n|    Sales|     30|   2|\n|       IT|     40|   2|\n+---------+-------+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark Where Filter Function | Multiple Conditions\n\ndeptDF.filter(deptDF.dept_id > 30).show()\n\n\nfrom pyspark.sql.functions import col\ndeptDF.filter(col('dept_id')>30).show()  #using col \n\ndeptDF.filter('dept_id > 20').show()  #sql exp\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"732c8762-107d-4ba5-bb02-5ac1e21e461c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|       IT|     40|\n+---------+-------+\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|       IT|     40|\n+---------+-------+\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#like and rlike \ndata2 = [(2,\"Michael Rose\"),(3,\"Robert Williams\"),\n     (4,\"Rames Rose\"),(5,\"Rames rose\")\n  ]\n\ndf2=spark.createDataFrame(data2,schema=['id','name'])\n\ndf2.filter(df2.name.like('%rose%')).show()  \n\ndf2.filter(df2.name.rlike('(?i)^*rose$')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"14be3173-de65-44c2-ac17-71b2a47a67bb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----------+\n| id|      name|\n+---+----------+\n|  5|Rames rose|\n+---+----------+\n\n+---+------------+\n| id|        name|\n+---+------------+\n|  2|Michael Rose|\n|  4|  Rames Rose|\n|  5|  Rames rose|\n+---+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark Distinct to Drop Duplicate Rows\n\n# Prepare Data\ndata = [(\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600), \\\n    (\"Robert\", \"Sales\", 4100), \\\n    (\"Maria\", \"Finance\", 3000), \\\n    (\"James\", \"Sales\", 3000), \\\n    (\"Scott\", \"Finance\", 3300), \\\n    (\"Jen\", \"Finance\", 3900), \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000), \\\n    (\"Saif\", \"Sales\", 4100) \\\n  ]\n\n\ncolumns = ['employee_name','department','salary']\n\ndf=spark.createDataFrame(data, schema=columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46dad276-5e72-4cc4-aaec-c4682f27d627","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["distinctDF = df.distinct()\nprint('Distinct count '+str(distinctDF.count()))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"193189f4-d640-4bf0-a32f-7bbafe7afdb1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Distinct count 9\n"]}],"execution_count":0},{"cell_type":"code","source":["df2=df.dropDuplicates()\nprint('Distinct count '+str(df2.count()))\ndf2.show()\n\ndropDisDF=df.dropDuplicates(['department','salary'])\nprint(dropDisDF.count())\n\ndropDisDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"539a7b18-11b1-42fd-865b-f1cac6effb7b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Distinct count 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        James|     Sales|  3000|\n|      Michael|     Sales|  4600|\n|       Robert|     Sales|  4100|\n|        Maria|   Finance|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|         Jeff| Marketing|  3000|\n|        Kumar| Marketing|  2000|\n|         Saif|     Sales|  4100|\n+-------------+----------+------+\n\n8\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|        Maria|   Finance|  3000|\n|        Scott|   Finance|  3300|\n|          Jen|   Finance|  3900|\n|        Kumar| Marketing|  2000|\n|         Jeff| Marketing|  3000|\n|        James|     Sales|  3000|\n|       Robert|     Sales|  4100|\n|      Michael|     Sales|  4600|\n+-------------+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark orderBy() and sort() explained\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n  ]\ncolumns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80edb990-bb32-4f3d-acb4-3dd6941de9f5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# df.sort('employee_name','department').show()\n# df.sort(df.department.asc(), df.state.desc()).show()\n\n# #DataFrame sorting using orderBy() function\n\n# df.orderBy(\"department\",\"state\").show(truncate=False)\n# df.orderBy(df.department.asc(),df.state.asc()).show()\n# df.orderBy(col('department').asc())\n\n\n#asc_nulls_first() and asc_nulls_last() and equivalent descending functions.\n\n# Using Raw SQL\n\ndf.createOrReplaceTempView('EMP')\nspark.sql('select * from EMP order by department desc').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d7568e5d-86ff-4f40-b433-f1146b24dd50","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        Scott|   Finance|   NY| 83000| 36|19000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|        Raman|   Finance|   CA| 99000| 40|24000|\n+-------------+----------+-----+------+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark Groupby Explained with Example\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n  ]\n\nschema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n#df = spark.createDataFrame(data=simpleData, schema = schema)\n#df.printSchema()\n#df.show(truncate=False)\n\ndf.groupBy('department').sum('salary').show()\ndf.groupBy('department').count().show()\ndf.groupBy('department').min('salary').show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"667506ce-a8aa-4d41-ad67-0b7f14c7cfa6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----------+\n|department|sum(salary)|\n+----------+-----------+\n|     Sales|     257000|\n|   Finance|     351000|\n| Marketing|     171000|\n+----------+-----------+\n\n+----------+-----+\n|department|count|\n+----------+-----+\n|     Sales|    3|\n|   Finance|    4|\n| Marketing|    2|\n+----------+-----+\n\n+----------+-----------+\n|department|min(salary)|\n+----------+-----------+\n|     Sales|      81000|\n|   Finance|      79000|\n| Marketing|      80000|\n+----------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Joins\n\nemp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n  ]\nempColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n       \"emp_dept_id\",\"gender\",\"salary\"]\n\nempDF = spark.createDataFrame(data=emp, schema = empColumns)\nempDF.printSchema()\nempDF.show(truncate=False)\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\nempDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id,'inner').show()\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f2a5961-bd0a-46ef-bc4f-ddd70d2b2b9f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark Union and UnionAll Explained\n\nimport pyspark\nfrom pyspark.sql import SparkSession\n\n# spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\n# simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n#     (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n#     (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n#     (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n#   ]\n\n# columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n# df = spark.createDataFrame(data = simpleData, schema = columns)\n# df.printSchema()\n# df.show(truncate=False)\n\n# simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n#     (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n#     (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n#     (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n#     (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n#   ]\n# columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n\n# df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n\n# df2.printSchema()\n# df2.show(truncate=False)\n\nunionDF = df.union(df2)\nunionDF.show(truncate=False)\ndf.unionAll(df2).show() ##Deprectaed , recomend to use union\n\ndisDF = df.union(df2).distinct()  #returns only distinct rows\ndisDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"802d3174-9528-4036-9142-75c57f674b0e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|        James|     Sales|   NY| 90000| 34|10000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|        James|     Sales|   NY| 90000| 34|10000|\n|      Michael|     Sales|   NY| 86000| 56|20000|\n|       Robert|     Sales|   CA| 81000| 30|23000|\n|        Maria|   Finance|   CA| 90000| 24|23000|\n|          Jen|   Finance|   NY| 79000| 53|15000|\n|         Jeff| Marketing|   CA| 80000| 25|18000|\n|        Kumar| Marketing|   NY| 91000| 50|21000|\n+-------------+----------+-----+------+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Union by name \n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\n# Create DataFrame df1 with columns name, and id\ndata = [(\"James\",34), (\"Michael\",56), \\\n        (\"Robert\",30), (\"Maria\",24) ]\n\ndf1 = spark.createDataFrame(data = data, schema=[\"name\",\"id\"])\ndf1.printSchema()\n\n# Create DataFrame df2 with columns name and id\ndata2=[(34,\"James\"),(45,\"Maria\"), \\\n       (45,\"Jen\"),(34,\"Jeff\")]\n\ndf2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\ndf2.printSchema()\n\n\nprint(df1.union(df2).show())\nprint(df1.unionByName(df2).show())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce1cb0c0-08ab-492a-a454-194dad6c9c7d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- id: long (nullable = true)\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n+-------+-----+\n|   name|   id|\n+-------+-----+\n|  James|   34|\n|Michael|   56|\n| Robert|   30|\n|  Maria|   24|\n|     34|James|\n|     45|Maria|\n|     45|  Jen|\n|     34| Jeff|\n+-------+-----+\n\nNone\n+-------+---+\n|   name| id|\n+-------+---+\n|  James| 34|\n|Michael| 56|\n| Robert| 30|\n|  Maria| 24|\n|  James| 34|\n|  Maria| 45|\n|    Jen| 45|\n|   Jeff| 34|\n+-------+---+\n\nNone\n"]}],"execution_count":0},{"cell_type":"code","source":["#transform \n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import upper\n\n# Create SparkSession\nspark = SparkSession.builder \\\n            .appName('SparkByExamples.com') \\\n            .getOrCreate()\n\n# Prepare Data\nsimpleData = ((\"Java\",4000,5), \\\n    (\"Python\", 4600,10),  \\\n    (\"Scala\", 4100,15),   \\\n    (\"Scala\", 4500,15),   \\\n    (\"PHP\", 3000,20),  \\\n  )\ncolumns= [\"CourseName\", \"fee\", \"discount\"]\n\n# Create DataFrame\ndf = spark.createDataFrame(data = simpleData, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)\n\n\ndef to_upper_str_columns(df):\n    return df.withColumn('CourseName',upper(df.CourseName))\n\ndef reduce_price(df, reduceBy):\n    return df.withColumn('new_fee',df.fee - reduceBy)\n\ndef apply_discount(df):\n    return df.withColumn('discounted_fee', df.new_fee - (df.new_fee * df.discount) / 100)\n\ndef select_col(df):\n    return df.select(\"CourseName\",\"discounted_fee\")\n\n\ndf2=df.transform(to_upper_str_columns) \\\n       .transform(reduce_price,100) \\\n       .transform(apply_discount) \\\n       .transform(select_col)\n\ndf2.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"42793d82-9f46-4616-b2ea-2d7978fc3c37","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- CourseName: string (nullable = true)\n |-- fee: long (nullable = true)\n |-- discount: long (nullable = true)\n\n+----------+----+--------+\n|CourseName|fee |discount|\n+----------+----+--------+\n|Java      |4000|5       |\n|Python    |4600|10      |\n|Scala     |4100|15      |\n|Scala     |4500|15      |\n|PHP       |3000|20      |\n+----------+----+--------+\n\n+----------+--------------+\n|CourseName|discounted_fee|\n+----------+--------------+\n|      JAVA|        3705.0|\n|    PYTHON|        4050.0|\n|     SCALA|        3400.0|\n|     SCALA|        3740.0|\n|       PHP|        2320.0|\n+----------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#tranform function \n## Create DataFrame with Array\ndata = [\n (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n]\ndf = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import transform\ndf.select(transform('Languages1', lambda x: upper(x)).alias('LANGUAGE')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef010aa8-c4e5-4efa-b899-2f3c0c626e75","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- Name: string (nullable = true)\n |-- Languages1: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- Languages2: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n+----------------+------------------+---------------+\n|            Name|        Languages1|     Languages2|\n+----------------+------------------+---------------+\n|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|\n|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|\n|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|\n+----------------+------------------+---------------+\n\n+------------------+\n|          LANGUAGE|\n+------------------+\n|[JAVA, SCALA, C++]|\n|[SPARK, JAVA, C++]|\n|      [CSHARP, VB]|\n+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Apply\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ncolumns = [\"Seqno\",\"Name\"]\ndata = [(\"1\", \"john jones\"),\n    (\"2\", \"tracey smith\"),\n    (\"3\", \"amy sanders\")]\n\ndf = spark.createDataFrame(data=data,schema=columns)\n\n# df.show(truncate=False)\n\n\n# df.withColumn('UPPER',upper(df.Name)).show()\n# df.select('Seqno', 'Name', upper(df.Name)).show()\n\n# df.createOrReplaceTempView('TAB')\n\n# spark.sql(\"Select Seqno, upper(Name) from TAB\").show()\n\n\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import StringType\n\ndef upperCase(str):\n    return str.upper()\n\nupperCaseUDF=udf(lambda x:upperCase(x),StringType())\n\n#custom udf with column \ndf.withColumn('Cureated Name', upperCaseUDF(df.Name)).show()\n\ndf.select('Seqno','Name',upperCaseUDF('Name').alias('UPPER_NAME')).show()\n\nspark.udf.register('upperCaseUDF',upperCaseUDF)\ndf.createOrReplaceTempView('TAB')\nspark.sql('select Seqno, Name, upperCaseUDF(Name) as UPPER_NAME from TAB').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0836dfd-5fff-4d21-97fc-a02c26de4575","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------------+-------------+\n|Seqno|        Name|Cureated Name|\n+-----+------------+-------------+\n|    1|  john jones|   JOHN JONES|\n|    2|tracey smith| TRACEY SMITH|\n|    3| amy sanders|  AMY SANDERS|\n+-----+------------+-------------+\n\n+-----+------------+------------+\n|Seqno|        Name|  UPPER_NAME|\n+-----+------------+------------+\n|    1|  john jones|  JOHN JONES|\n|    2|tracey smith|TRACEY SMITH|\n|    3| amy sanders| AMY SANDERS|\n+-----+------------+------------+\n\n+-----+------------+------------+\n|Seqno|        Name|  UPPER_NAME|\n+-----+------------+------------+\n|    1|  john jones|  JOHN JONES|\n|    2|tracey smith|TRACEY SMITH|\n|    3| amy sanders| AMY SANDERS|\n+-----+------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Imports\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f65b7f65-29c5-4434-abfd-f6803f0cc324","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["       Fee  Discount\n0  20000.0      1000\n1  25000.0      2500\n2  30000.0      1500\n3  22000.0      1200\n4      NaN      3000\n0    21000.0\n1    27500.0\n2    31500.0\n3    23200.0\n4        NaN\ndtype: float64\n"]}],"execution_count":0},{"cell_type":"code","source":["#Flatmap  in rdd\n\ndata = [\"Project Gutenberg’s\",\n        \"Alice’s Adventures in Wonderland\",\n        \"Project Gutenberg’s\",\n        \"Adventures in Wonderland\",\n        \"Project Gutenberg’s\"]\n\nrdd=spark.sparkContext.parallelize(data)\n\n# for ele in rdd.collect():\n#     #print(ele)\n    \n# rdd2=rdd.flatMap(lambda x: x.split(' '))\n# for ele in rdd2.collect():\n#     print(ele)\n    \n    \n#explode in Dataframe \n\narrayData = [\n        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n        ('Washington',None,None),\n        ('Jefferson',['1','2'],{})]\ndf = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import explode\ndf2=df.select(df.name, explode(df.knownLanguages))\ndf2.printSchema()\ndf2.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc421814-b7df-47f0-9045-a35ab1a92954","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-------------------+-----------------------------+\n|name      |knownLanguages     |properties                   |\n+----------+-------------------+-----------------------------+\n|James     |[Java, Scala]      |{eye -> brown, hair -> black}|\n|Michael   |[Spark, Java, null]|{eye -> null, hair -> brown} |\n|Robert    |[CSharp, ]         |{eye -> , hair -> red}       |\n|Washington|null               |null                         |\n|Jefferson |[1, 2]             |{}                           |\n+----------+-------------------+-----------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- col: string (nullable = true)\n\n+---------+------+\n|     name|   col|\n+---------+------+\n|    James|  Java|\n|    James| Scala|\n|  Michael| Spark|\n|  Michael|  Java|\n|  Michael|  null|\n|   Robert|CSharp|\n|   Robert|      |\n|Jefferson|     1|\n|Jefferson|     2|\n+---------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Pyspark UDF\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import StringType\n\n\ncolumn = ['seqNo', 'Name']\ndata = [('1', 'john jones'),('2', 'tracey Smitj'),('3', 'amy snaders')]\n\ndf=spark.createDataFrame(data,schema=column)\ndf.show()\n\n\ndef  converCase(str):\n    resStr=''\n    arr=str.split(' ')\n    for x in arr:\n        resStr = resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n    return resStr\n\n\nconverUdf=udf(lambda z: converCase(z))\n\ndf.select('seqNo', converUdf('Name').alias('UDF_name')).show()\n\n@udf\ndef upperCase(str):\n    return str.upper()\n\ndf.withColumn(\"New Name\", upperCase('Name')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec7b3fac-b13c-4d82-9fb1-8a16b60a4d59","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------------+\n|seqNo|        Name|\n+-----+------------+\n|    1|  john jones|\n|    2|tracey Smitj|\n|    3| amy snaders|\n+-----+------------+\n\n+-----+-------------+\n|seqNo|     UDF_name|\n+-----+-------------+\n|    1|  John Jones |\n|    2|Tracey Smitj |\n|    3| Amy Snaders |\n+-----+-------------+\n\n+-----+------------+------------+\n|seqNo|        Name|    New Name|\n+-----+------------+------------+\n|    1|  john jones|  JOHN JONES|\n|    2|tracey Smitj|TRACEY SMITJ|\n|    3| amy snaders| AMY SNADERS|\n+-----+------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark foreach() Usage with Examples\n\ndef foreach_func(df):\n    print(df.Name)\n    \ndf.foreach(foreach_func)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a021b5ee-52cd-4f26-a507-b539c0891678","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#PySpark – sample() vs sampleBy()\n\n# sample() function synatx\n\n# # sample(withReplacement, fraction, seed=None)\n# fraction – Fraction of rows to generate, range [0.0, 1.0]. Note that it doesn’t guarantee to provide the exact number of the fraction of records.\n# seed – Seed for sampling (default a random seed). Used to reproduce the same random sampling.\n\n# withReplacement – Sample with replacement or not (default False).\n\ndf=spark.range(100)\n\nprint(df.sample(True,0.06,123).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ff6ed93-4921-4bb4-b24d-de64b26370ad","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[Row(id=37), Row(id=46), Row(id=61), Row(id=65), Row(id=81), Row(id=91), Row(id=92), Row(id=92)]\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark fillna() & fill() – Replace NULL/None Values\n\n#PySpark Pivot and Unpivot DataFrame\n\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\n#Create spark session\ndata = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n\ncolumns= [\"Product\",\"Amount\",\"Country\"]\n# df = spark.createDataFrame(data = data, schema = columns)\n# df.printSchema()\n# df.show(truncate=False)\n\ndf_pivot=df.groupby('Product').pivot('Country').sum('Amount')\ndf_pivot.show()\n\nfrom pyspark.sql.functions import expr\n\nunpivot_expr = \"stack(3,'Canada',Canada,'China',China,'Mexico',Mexico) as (Country,Amount)\"\nunpivot_df = df_pivot.select('Product',expr(unpivot_expr))\nunpivot_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7dbb4b4c-843a-4fb0-b56f-5fb5cdea1b5d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+------+-----+------+----+\n|Product|Canada|China|Mexico| USA|\n+-------+------+-----+------+----+\n| Orange|  null| 4000|  null|4000|\n|  Beans|  null| 1500|  2000|1600|\n| Banana|  2000|  400|  null|1000|\n|Carrots|  2000| 1200|  null|1500|\n+-------+------+-----+------+----+\n\n+-------+-------+------+\n|Product|Country|Amount|\n+-------+-------+------+\n| Orange| Canada|  null|\n| Orange|  China|  4000|\n| Orange| Mexico|  null|\n|  Beans| Canada|  null|\n|  Beans|  China|  1500|\n|  Beans| Mexico|  2000|\n| Banana| Canada|  2000|\n| Banana|  China|   400|\n| Banana| Mexico|  null|\n|Carrots| Canada|  2000|\n|Carrots|  China|  1200|\n|Carrots| Mexico|  null|\n+-------+-------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#example of pivot\ndata1  = [{'Name':'Jhon','ID':21.528,'Add':'USA'},{'Name':'Joe','ID':3.69,'Add':'USA'},{'Name':'Tina','ID':2.48,'Add':'IND'},{'Name':'Jhon','ID':22.22, 'Add':'USA'},{'Name':'Joe','ID':5.33,'Add':'INA'}]\n\ndf_ex_1 =spark.createDataFrame(data1)\ndf_ex_1.show()\n\ndf_ex_1_pivot=df_ex_1.groupby('Name').pivot('Add').count()\ndf_ex_1_pivot.show()\n\ndf_ex_1_unpivot =df_ex_1_pivot.select('Name',expr(\"stack(3, 'INA',INA,'IND',IND,'USA',USA) as (Add,Count)\"))\ndf_ex_1_unpivot.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee9f3fee-3e1e-4155-874a-e63305f86498","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------+----+\n|Add|    ID|Name|\n+---+------+----+\n|USA|21.528|Jhon|\n|USA|  3.69| Joe|\n|IND|  2.48|Tina|\n|USA| 22.22|Jhon|\n|INA|  5.33| Joe|\n+---+------+----+\n\n+----+----+----+----+\n|Name| INA| IND| USA|\n+----+----+----+----+\n| Joe|   1|null|   1|\n|Jhon|null|null|   2|\n|Tina|null|   1|null|\n+----+----+----+----+\n\n+----+---+-----+\n|Name|Add|Count|\n+----+---+-----+\n| Joe|INA|    1|\n| Joe|IND| null|\n| Joe|USA|    1|\n|Jhon|INA| null|\n|Jhon|IND| null|\n|Jhon|USA|    2|\n|Tina|INA| null|\n|Tina|IND|    1|\n|Tina|USA| null|\n+----+---+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark MapType (Dict) Usage with Examples\n\nfrom pyspark.sql.types import StructField, StructType, StringType, MapType\nschema = StructType([\n    StructField('name', StringType(), True),\n    StructField('properties', MapType(StringType(), StringType()), True)\n])\n\ndataDict = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(dataDict, schema=schema)\n\n\n# df3 = df.rdd.map(lambda x: (x.name, x.properties['hair'], x.properties['eye'])).toDF(['name','hair','eye'])\n# df3.printSchema()\n# df3.show()\n\ndf.withColumn('hair',df.properties.getItem('hair')) \\\n  .withColumn('eye', df.properties.getItem('eye')) \\\n  .drop('properties').show()\n\nfrom pyspark.sql.functions import explode\ndf.select('name', explode('properties')).show()\ndf.show()\n# df.show()\n\n# df.printSchema()\n# df.show(truncate=False)\n\n\nfrom pyspark.sql.functions import map_keys\ndf.select(df.name, map_keys(df.properties)).show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a9cee42-f975-46ef-b1b5-93f520fbf524","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n+----------+----+-----+\n|      name| key|value|\n+----------+----+-----+\n|     James| eye|brown|\n|     James|hair|black|\n|   Michael| eye| null|\n|   Michael|hair|brown|\n|    Robert| eye|black|\n|    Robert|hair|  red|\n|Washington| eye| grey|\n|Washington|hair| grey|\n| Jefferson| eye|     |\n| Jefferson|hair|brown|\n+----------+----+-----+\n\n+----------+--------------------+\n|      name|          properties|\n+----------+--------------------+\n|     James|{eye -> brown, ha...|\n|   Michael|{eye -> null, hai...|\n|    Robert|{eye -> black, ha...|\n|Washington|{eye -> grey, hai...|\n| Jefferson|{eye -> , hair ->...|\n+----------+--------------------+\n\n+----------+--------------------+\n|      name|map_keys(properties)|\n+----------+--------------------+\n|     James|         [eye, hair]|\n|   Michael|         [eye, hair]|\n|    Robert|         [eye, hair]|\n|Washington|         [eye, hair]|\n| Jefferson|         [eye, hair]|\n+----------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#PySpark Aggregate Functions with Examples"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69f67d63-a625-4ad2-8b7a-874b8cf85af0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["simpleData = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\nschema = [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import approx_count_distinct, avg, collect_list, collect_set, countDistinct, count, first, last\n\n# print(\"approx_count_distinct\" + str(df.select(approx_count_distinct('salary')).collect()[0][0]))\n# print(\"avg\" + str(df.select(avg('salary')).collect()[0][0]))\n# df.select(collect_list('salary')).show(truncate=False)\n# df.select(avg('salary')).show()\n\n# df.select(collect_set('salary')).show(truncate=False)\n\ndf.select(countDistinct('department')).show()\ndf.select(count('salary')).show()\n\ndf.select(first('salary')).show()\ndf.select(last('salary')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b67e533-7d04-41a4-b80a-683a2bf7813e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n+--------------------------+\n|count(DISTINCT department)|\n+--------------------------+\n|                         3|\n+--------------------------+\n\n+-------------+\n|count(salary)|\n+-------------+\n|           10|\n+-------------+\n\n+-------------+\n|first(salary)|\n+-------------+\n|         3000|\n+-------------+\n\n+------------+\n|last(salary)|\n+------------+\n|        4100|\n+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#windows function\n\nsimpleData = ((\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600),  \\\n    (\"Robert\", \"Sales\", 4100),   \\\n    (\"Maria\", \"Finance\", 3000),  \\\n    (\"James\", \"Sales\", 3000),    \\\n    (\"Scott\", \"Finance\", 3300),  \\\n    (\"Jen\", \"Finance\", 3900),    \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000),\\\n    (\"Saif\", \"Sales\", 4100) \\\n  )\n \ncolumns= [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data = simpleData, schema = columns)\n# df.printSchema()\n# df.show(truncate=False)\n\n#row_number window function\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\n#windowSpec = Window.partitionBy('department').orderBy('salary')\n\n# df.withColumn('row_num', row_number().over(windowSpec)).show(truncate=False)\n\n#rank() \n\nfrom pyspark.sql.functions import rank, dense_rank\n\n# windowSpec = Window.partitionBy('department').orderBy('salary')\n# df.withColumn('rank',  rank().over(windowSpec)).show()\n# df.withColumn('rank',  dense_rank().over(windowSpec)).show()\n\n\n#percent_rank \n\nfrom pyspark.sql.functions import percent_rank\n\n# windowSpec = Window.partitionBy('department').orderBy('Salary')\n# df.withColumn('percent_rank', percent_rank().over(windowSpec)).show()\n\n#ntile : returns the relative rank of result rows within a window partition\n\nfrom pyspark.sql.functions import ntile\n\n# windowSpec = Window.partitionBy('department').orderBy('salary')\n# df.withColumn('ntile', ntile(1).over(windowSpec)).show()\n\n\n# cume_dist() window function is used to get the cumulative distribution of values within a window partition.\n\n# from pyspark.sql.functions import cume_dist\n\n# df.withColumn('cume_dist', cume_dist().over(windowSpec)).show()\n\n#Lag \n\n# from pyspark.sql.functions import lag, lead\n\n# #df.withColumn('lag', lag('Salary',2).over(windowSpec)).show()\n\n# df.withColumn('lead',lead('salary', 1).over(windowSpec)).show()\n\n\n#PySpark Window Aggregate Functions\n\nwindowSpecAgg = Window.partitionBy('department')\nfrom pyspark.sql.functions import col, avg, sum, min, max, row_number\n\ndf.withColumn('row', row_number().over(windowSpec)) \\\n  .withColumn('avg', avg('salary').over(windowSpecAgg)) \\\n  .withColumn('sum', sum('salary').over(windowSpecAgg)) \\\n  .withColumn('min', min('salary').over(windowSpecAgg)) \\\n  .withColumn('max', max('salary').over(windowSpecAgg)) \\\n  .where(col('row')=='1').select('department', 'avg', 'min', 'max') \\\n  .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"43e40365-f9cd-429f-a279-6830b9a65838","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+------+----+----+\n|department|avg   |min |max |\n+----------+------+----+----+\n|Finance   |3400.0|3000|3900|\n|Marketing |2500.0|2000|3000|\n|Sales     |3760.0|3000|4600|\n+----------+------+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark SQL Date and Timestamp Functions\n\nfrom pyspark.sql.functions import *\n\n# data = [['1','2020-02-01'],['2','2019-03-01'],['3', '2021-03-01']]\n# df=spark.createDataFrame(data,schema=['id','input'])\n# df.show()\n# df.printSchema()\n\n\n##current_date\ndf.select(current_date().alias('Current_date')).show(1)\n\n#date_format()\n\n# df.select(date_format(('input'), 'yyyy-dd-MM')).printSchema()\n\n\n# #to_date --converts string type to date type \n\n# df.select(to_date('input', 'yyyy-dd-mm')).printSchema()\n\n\n\n# #datediff\n\n# df.select('input' , datediff('input', current_date())).show()\n\n#months_between\n\n# df.select('input', months_between(current_date(), 'input').alias('months_between')).show()\n\n#trunc \n\n# df.select('input', \n#           trunc('input', 'Month').alias('Month_trunc'),\n#           trunc('input', 'year').alias('year_trunc'),\n#           trunc('input', 'date').alias('date_trunc')\n#          ).show()\n\n\n#add_months() , date_add(), date_sub()\n\n# df.select('input', \n#           add_months('input', 2).alias('add_months'),\n#          add_months('input', -2).alias('sub_months'),\n#          date_add('input', 20).alias('date_add'),\n#          date_sub('input', 10).alias('date_sub')).show()\n\n#year(), month(), month(),next_day(), weekofyear()\n\n# df.select('input' \\\n#           , year('input').alias('year') \\\n#          , month('input').alias('month') \\\n#          , next_day('input', dayOfWeek='Tue').alias('next_day') \\\n#          , weekofyear('input').alias('week_year') \\\n#          ).show()\n\n\n# dayofweek(), dayofmonth(), dayofyear()\n\ndf.select('input', \n          dayofweek('input').alias('dayOfWeek'),\n          dayofmonth('input').alias('dayofmonth'),\n         dayofyear('input').alias('dayOfYear')).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff587872-97dd-450c-a26c-ab599d08d3e1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------+\n|Current_date|\n+------------+\n|  2023-02-22|\n+------------+\nonly showing top 1 row\n\n+----------+---------+----------+---------+\n|     input|dayOfWeek|dayofmonth|dayOfYear|\n+----------+---------+----------+---------+\n|2020-02-01|        7|         1|       32|\n|2019-03-01|        6|         1|       60|\n|2021-03-01|        2|         1|       60|\n+----------+---------+----------+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#current_timestamp() , to_timestamp\n\n# data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n# df2=spark.createDataFrame(data,[\"id\",\"input\"])\n# df2.show(truncate=False)\n\ndf2.select('input', \n           current_timestamp().alias('currentTime'),\n           to_timestamp('input', 'MM-dd-yyyy HH mm ss SSSS').alias('to_timestamp')).show(truncate=False)\n\n\n# hour(), Minute() and second()\n\ndata=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\ndf3=spark.createDataFrame(data,[\"id\",\"input\"])\ndf3.printSchema()\n\ndf3.select('input', hour('input'), minute('input'), second('input')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"269f930a-4596-435a-87ea-dcb4d594f23c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------------+-----------------------+-----------------------+\n|input                  |currentTime            |to_timestamp           |\n+-----------------------+-----------------------+-----------------------+\n|02-01-2020 11 01 19 06 |2023-02-22 08:08:23.446|2020-02-01 11:01:19.06 |\n|03-01-2019 12 01 19 406|2023-02-22 08:08:23.446|2019-03-01 12:01:19.406|\n|03-01-2021 12 01 19 406|2023-02-22 08:08:23.446|2021-03-01 12:01:19.406|\n+-----------------------+-----------------------+-----------------------+\n\nroot\n |-- id: string (nullable = true)\n |-- input: string (nullable = true)\n\n+--------------------+-----------+-------------+-------------+\n|               input|hour(input)|minute(input)|second(input)|\n+--------------------+-----------+-------------+-------------+\n|2020-02-01 11:01:...|         11|            1|           19|\n|2019-03-01 12:01:...|         12|            1|           19|\n|2021-03-01 12:01:...|         12|            1|           19|\n+--------------------+-----------+-------------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#PySpark JSON Functions with Examples\nfrom pyspark.sql import SparkSession,Row\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\njsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\ndf=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])  ##creadte dateframe of json \n# df.show(truncate=False)\n\n\n#from_json\n# from pyspark.sql.types import StringType,StructField,StructType,MapType\n# df2=df.withColumn('value', from_json('value', MapType(StringType(), StringType())))\n# df2.show(truncate=False)\n# df2.printSchema()\n\n\n# #to_json\n\n# df3=df2.select(to_json('value'))\n# df3.show()\n\n#json_tuple\n# df.select(json_tuple('value', 'Zipcode','ZipCodeType','City')).toDF('Zipcode','ZipCodeType','City').show()\n\n#get_json object\nfrom pyspark.sql.functions import get_json_object\n\ndf.select('id', get_json_object('value', \"$.City\").alias('city')).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ddf829cf-e9a1-44a5-a50f-6c3b2c83d39b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----------+\n|id |city       |\n+---+-----------+\n|1  |PARC PARQUE|\n+---+-----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["##PySpark Datasources"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3dc93297-c258-4ec3-867b-bacf7286b7f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["csvdf=spark.read.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0fd15946-a2b9-47d5-827f-e64e2db1754e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"quickstart_df","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2443035618392741}},"nbformat":4,"nbformat_minor":0}
